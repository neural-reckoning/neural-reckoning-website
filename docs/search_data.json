{
  "goodman_thesis": {
    "year": 2006,
    "authors": "Goodman DFM",
    "title": "Boundaries of slices of quasifuchsian space",
    "publisher": "University of Warwick",
    "phd_thesis": true,
    "categories": [
      "Mathematics",
      "Software"
    ],
    "urls": [
      [
        "Thesis (PDF)",
        "https://neural-reckoning.org/dan-goodman-thesis.pdf"
      ]
    ],
    "abstract": "We prove that the boundaries of the Maskit and Bers slices contain an uncountable, dense set of points about which the boundary spirals infinitely. The set of points about which we prove the boundary spirals infinitely has zero measure and is akin to a countable union of Cantor sets. On the basis of strong numerical evidence, we conjecture that in fact the boundary spirals infinitely at almost all points in the boundary. We further conjecture that the Hausdorff dimension of the Maskit slice is less than 1.25."
  },
  "spirals": {
    "year": 2006,
    "authors": "Goodman D",
    "title": "Spirals in the boundaries of slices of quasifuchsian space",
    "journal": "Conformal Geometry and Dynamics",
    "additional": "10",
    "categories": [
      "Mathematics"
    ],
    "urls": [
      [
        "Journal",
        "https://doi.org/10.1090/S1088-4173-06-00133-0"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/ygz9demdn5n5l19/spirals-paper.pdf?dl=1"
      ]
    ],
    "abstract": "We prove that the Bers and Maskit slices of the quasi-Fuchsian space of\na once-punctured torus have a dense, uncountable set of points in their\nboundaries about which the boundary spirals infinitely."
  },
  "brian": {
    "year": 2008,
    "authors": "Goodman D, Brette R",
    "title": "Brian: a simulator for spiking neural networks in Python",
    "journal": "Frontiers in Neuroinformatics",
    "additional": "2(5)",
    "doi": "10.3389/neuro.11.005.2008",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://journal.frontiersin.org/Journal/10.3389/neuro.11.005.2008/abstract"
      ]
    ],
    "abstract": "\"Brian\" is a new simulator for spiking neural networks, written in\nPython (http://www.briansimulator.org ). It is an intuitive and highly\nflexible tool for rapidly developing new models, especially networks of\nsingle-compartment neurons. In addition to using standard types of\nneuron models, users can define models by writing arbitrary differential\nequations in ordinary mathematical notation. Python scientific libraries\ncan also be used for defining models and analysing data. Vectorisation\ntechniques allow efficient simulations despite the overheads of an\ninterpreted language. Brian will be especially valuable for working on\nnon-standard neuron models not easily covered by existing software, and\nas an alternative to using Matlab or C for simulations. With its easy\nand intuitive syntax, Brian is also very well suited for teaching\ncomputational neuroscience. ",
    "software": [
      "Brian"
    ]
  },
  "brian_focussed_review": {
    "selected": false,
    "year": 2009,
    "authors": "Goodman DFM, Brette R",
    "title": "The Brian simulator",
    "journal": "Frontiers in Neuroscience",
    "additional": "3(2)",
    "doi": "10.3389/neuro.01.026.2009",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://journal.frontiersin.org/Journal/10.3389/neuro.01.026.2009/abstract"
      ]
    ],
    "abstract": "\"Brian\" is a simulator for spiking neural networks\n(http://www.briansimulator.org ). The focus is on making the writing of\nsimulation code as quick and easy as possible for the user, and on\nflexibility: new and non-standard models are no more difficult to define\nthan standard ones. This allows scientists to spend more time on the\ndetails of their models, and less on their implementation. Neuron models\nare defined by writing differential equations in standard mathematical\nnotation, facilitating scientific communication. Brian is written in the\nPython programming language, and uses vector-based computation to allow\nfor efficient simulations. It is particularly useful for neuroscientific\nmodelling at the systems level, and for teaching computational\nneuroscience. ",
    "software": [
      "Brian"
    ]
  },
  "brian_neuromorphic_engineer": {
    "year": 2009,
    "authors": "Brette R, Goodman D",
    "title": "Brian: a simple and flexible simulator for spiking neural networks",
    "journal": "The Neuromorphic Engineer",
    "additional_detail": "Note: The Neuromorphic Engineer journal appears to have closed down, so only a direct link to a\nPDF of the original article is included below.",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "PDF",
        "https://www.dropbox.com/s/ri7yhwzqk318ff4/brian-neuromorphic-engineer.pdf?dl=0"
      ]
    ],
    "peer_reviewed": false,
    "software": [
      "Brian"
    ]
  },
  "codegen": {
    "year": 2010,
    "authors": "Goodman DFM",
    "title": "Code Generation: A Strategy for Neural Network Simulators",
    "journal": "Neuroinformatics",
    "additional": "8, no. 3 (9).",
    "doi": "10.1007/s12021-010-9082-x",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal",
        "https://link.springer.com/article/10.1007/s12021-010-9082-x"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/si4uaau33csqejk/codegen.pdf?dl=1"
      ]
    ],
    "abstract": "We demonstrate a technique for the design of neural network simulation\nsoftware, runtime code generation. This technique can be used to give\nthe user complete flexibility in specifying the mathematical model for\ntheir simulation in a high level way, along with the speed of code\nwritten in a low level language such as C++. It can also be used to\nwrite code only once but target different hardware platforms, including\ninexpensive high performance graphics processing units (GPUs). Code\ngeneration can be naturally combined with computer algebra systems to\nprovide further simplification and optimisation of the generated code.\nThe technique is quite general and could be applied to any simulation\npackage. We demonstrate it with the \"Brian\" simulator\n(http://www.briansimulator.org).",
    "software": [
      "Brian"
    ]
  },
  "learning_localisation": {
    "year": 2010,
    "authors": "Goodman DFM, Brette R",
    "title": "Learning to localise sounds with spiking neural networks",
    "journal": "Advances in Neural Information Processing Systems",
    "additional": "23",
    "categories": [
      "Sound localisation",
      "Modelling",
      "Spiking",
      "Learning",
      "Plasticity"
    ],
    "urls": [
      [
        "Journal",
        "http://papers.nips.cc/paper/4127-learning-to-localise-sounds-with-spiking-neural-networks"
      ],
      [
        "PDF",
        "http://papers.nips.cc/paper/4127-learning-to-localise-sounds-with-spiking-neural-networks.pdf"
      ],
      [
        "BibTeX",
        "http://papers.nips.cc/paper/4127-learning-to-localise-sounds-with-spiking-neural-networks/bibtex"
      ]
    ],
    "abstract": "To localise the source of a sound, we use location-specific properties\nof the signals received at the two ears caused by the asymmetric\nfiltering of the original sound by our head and pinnae, the head-related\ntransfer functions (HRTFs). These HRTFs change throughout an organism's\nlifetime, during development for example, and so the required neural\ncircuitry cannot be entirely hardwired. Since HRTFs are not directly\naccessible from perceptual experience, they can only be inferred from\nfiltered sounds. We present a spiking neural network model of sound\nlocalisation based on extracting location-specific synchrony patterns,\nand a simple supervised algorithm to learn the mapping between synchrony\npatterns and locations from a set of example sounds, with no previous\nknowledge of HRTFs. After learning, our model was able to accurately\nlocalise new sounds in both azimuth and elevation, including the\ndifficult task of distinguishing sounds coming from the front and back.",
    "software": [
      "Brian"
    ]
  },
  "modelfitting": {
    "year": 2010,
    "authors": "Rossant C, Goodman DFM, Platkiewicz J, Brette R",
    "title": "Automatic fitting of spiking neuron models to electrophysiological recordings",
    "journal": "Frontiers in Neuroinformatics",
    "doi": "10.3389/neuro.11.002.2010",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://journal.frontiersin.org/Journal/10.3389/neuro.11.002.2010/abstract"
      ]
    ],
    "abstract": "Spiking models can accurately predict the spike trains produced by\ncortical neurons in response to somatically injected currents. Since the\nspecific characteristics of the model depend on the neuron, a\ncomputational method is required to fit models to electrophysiological\nrecordings. The fitting procedure can be very time consuming both in\nterms of computer simulations and in terms of code writing. We present\nalgorithms to fit spiking models to electrophysiological data\n(time-varying input and spike trains) that can run in parallel on\ngraphics processing units (GPUs). The model fitting library is\ninterfaced with Brian, a neural network simulator in Python. If a GPU is\npresent it uses just-in-time compilation to translate model equations\ninto optimized code. Arbitrary models can then be defined at script\nlevel and run on the graphics card. This tool can be used to obtain\nempirically validated spiking models of neurons in various systems. We\ndemonstrate its use on public data from the INCF Quantitative\nSingle-Neuron Modeling 2009 competition by comparing the performance of\na number of neuron spiking models. ",
    "software": [
      "Brian"
    ]
  },
  "qr": {
    "year": 2010,
    "authors": "Fletcher A, Goodman DFM",
    "title": "Quasiregular mappings of polynomial type in R<sup>2</sup>",
    "journal": "Conformal Geometry and Dynamics",
    "additional": "14",
    "categories": [
      "Mathematics"
    ],
    "urls": [
      [
        "Journal",
        "https://doi.org/10.1090/S1088-4173-2010-00219-5"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/v6h2bex5y09uqej/quadqr.pdf?dl=1"
      ]
    ],
    "abstract": "Complex dynamics deals with the iteration of holomorphic functions. As\nis well known, the first functions to be studied which gave non-trivial\ndynamics were quadratic polynomials, which produced beautiful computer\ngenerated pictures of Julia sets and the Mandelbrot set. In the same\nspirit, this article aims to study the dynamics of the simplest\nnon-trivial quasiregular mappings. These are mappings in R<sup>2</sup>\nwhich are a composition of a quadratic polynomial and an affine stretch. "
  },
  "spike_timing_sound_loc": {
    "year": 2010,
    "authors": "Goodman DFM, Brette R",
    "title": "Spike-timing-based computation in sound localization",
    "journal": "PLoS Computational Biology",
    "additional": "6(11): e1000993",
    "doi": "10.1371/journal.pcbi.1000993",
    "categories": [
      "Sound localisation",
      "Modelling",
      "Spiking"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1000993"
      ],
      [
        "PDF",
        "http://www.ploscompbiol.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1000993&representation=PDF"
      ],
      [
        "Code on ModelDB",
        "http://senselab.med.yale.edu/ModelDB/ShowModel.asp?model=126465"
      ]
    ],
    "abstract": "Spike timing is precise in the auditory system and it has been argued\nthat it conveys information about auditory stimuli, in particular about\nthe location of a sound source. However, beyond simple time differences,\nthe way in which neurons might extract this information is unclear and\nthe potential computational advantages are unknown. The computational\ndifficulty of this task for an animal is to locate the source of an\nunexpected sound from two monaural signals that are highly dependent on\nthe unknown source signal. In neuron models consisting of\nspectro-temporal filtering and spiking nonlinearity, we found that the\nbinaural structure induced by spatialized sounds is mapped to synchrony\npatterns that depend on source location rather than on source signal.\nLocation-specific synchrony patterns would then result in the activation\nof location-specific assemblies of postsynaptic neurons. We designed a\nspiking neuron model which exploited this principle to locate a variety\nof sound sources in a virtual acoustic environment using measured human\nhead-related transfer functions. The model was able to accurately\nestimate the location of previously unknown sounds in both azimuth and\nelevation (including front/back discrimination) in a known acoustic\nenvironment. We found that multiple representations of different\nacoustic environments could coexist as sets of overlapping neural\nassemblies which could be associated with spatial locations by Hebbian\nlearning. The model demonstrates the computational relevance of relative\nspike timing to extract spatial information about sources independently\nof the source signal.",
    "software": [
      "Brian"
    ]
  },
  "brian_hears": {
    "year": 2011,
    "authors": "Fontaine B, Goodman DFM, Benichoux V, Brette R",
    "title": "Brian Hears: online auditory processing using vectorisation over channels",
    "journal": "Frontiers in Neuroinformatics",
    "additional": "5:9",
    "doi": "10.3389/fninf.2011.00009",
    "categories": [
      "Brian",
      "Auditory"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://journal.frontiersin.org/Journal/10.3389/fninf.2011.00009/abstract"
      ]
    ],
    "abstract": "The human cochlea includes about 3000 inner hair cells which filter\nsounds at frequencies between 20 Hz and 20 kHz. This massively parallel\nfrequency analysis is reflected in models of auditory processing, which\nare often based on banks of filters. However, existing implementations\ndo not exploit this parallelism. Here we propose algorithms to simulate\nthese models by vectorizing computation over frequency channels, which\nare implemented in \"Brian Hears,\" a library for the spiking neural\nnetwork simulator package \"Brian.\" This approach allows us to use\nhigh-level programming languages such as Python, because with vectorized\noperations, the computational cost of interpretation represents a small\nfraction of the total cost. This makes it possible to define and\nsimulate complex models in a simple way, while all previous\nimplementations were model-specific. In addition, we show that these\nalgorithms can be naturally parallelized using graphics processing\nunits, yielding substantial speed improvements. We demonstrate these\nalgorithms with several state-of-the-art cochlear models, and show that\nthey compare favorably with existing, less flexible, implementations.",
    "software": [
      "Brian"
    ]
  },
  "modelfitting_focussed_review": {
    "year": 2011,
    "authors": "Rossant C, Goodman DFM, Fontaine B, Platkiewicz J, Magnusson AK, Brette R",
    "title": "Fitting neuron models to spike trains",
    "journal": "Frontiers in Neuroscience",
    "additional": "5:9",
    "doi": "10.3389/fnins.2011.00009",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://www.frontiersin.org/Neuroscience/10.3389/fnins.2011.00009/abstract"
      ]
    ],
    "abstract": "Computational modeling is increasingly used to understand the function\nof neural circuits in systems neuroscience. These studies require\nmodels of individual neurons with realistic input-output properties.\nRecently, it was found that spiking models can accurately predict the\nprecisely timed spike trains produced by cortical neurons in response to\nsomatically injected currents, if properly fitted. This requires fitting\ntechniques that are efficient and flexible enough to easily test\ndifferent candidate models. We present a generic solution, based on the\nBrian simulator (a neural network simulator in Python), which allows the\nuser to define and fit arbitrary neuron models to electrophysiological\nrecordings. It relies on vectorization and parallel computing techniques\nto achieve efficiency. We demonstrate its use on neural recordings in\nthe barrel cortex and in the auditory brainstem, and confirm that simple\nadaptive spiking models can accurately predict the response of cortical\nneurons. Finally, we show how a complex multicompartmental model can be\nreduced to a simple effective spiking model.",
    "software": [
      "Brian"
    ]
  },
  "rat_barrel_cortex": {
    "year": 2011,
    "authors": "Kremer Y, Léger J-F, Goodman D, Brette R, Bourdieu L",
    "title": "Late emergence of the vibrissa direction selectivity map in the rat barrel cortex",
    "journal": "Journal of Neuroscience",
    "additional": "31(29)",
    "doi": "10.1523/JNEUROSCI.6541-10.2011",
    "categories": [
      "Sensory",
      "Modelling",
      "Spiking",
      "Plasticity"
    ],
    "urls": [
      [
        "Journal",
        "http://www.jneurosci.org/content/31/29/10689.abstract"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/ypos545ptv0blf4/barrelcortex-preprint.pdf?dl=1"
      ]
    ],
    "abstract": "In the neocortex, neuronal selectivities for multiple sensorimotor\nmodalities are often distributed in topographical maps thought to emerge\nduring a restricted period in early postnatal development. Rodent barrel\ncortex contains a somatotopic map for vibrissa identity, but the\nexistence of maps representing other tactile features has not been\nclearly demonstrated. We addressed the issue of the existence in the rat\ncortex of an intrabarrel map for vibrissa movement direction using in\nvivo two-photon imaging. We discovered that the emergence of a direction\nmap in rat barrel cortex occurs long after all known critical periods\nin the somatosensory system. This map is remarkably specific, taking a\npinwheel-like form centered near the barrel center and aligned to the\nbarrel cortex somatotopy. We suggest that this map may arise from\nintracortical mechanisms and demonstrate by simulation that the\ncombination of spike-timing-dependent plasticity at synapses between\nlayer 4 and layer 2/3 and realistic pad stimulation is sufficient to\nproduce such a map. Its late emergence long after other classical maps\nsuggests that experience-dependent map formation and refinement continue\nthroughout adult life. ",
    "software": [
      "Brian"
    ]
  },
  "vectorised_algorithms": {
    "year": 2011,
    "authors": "Brette R, Goodman DFM",
    "title": "Vectorised algorithms for spiking neural network simulation",
    "journal": "Neural Computation",
    "additional": "23:6",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal",
        "http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00123?journalCode=neco"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/okq0bywihietbgr/algorithms-preprint.pdf?dl=1"
      ]
    ],
    "abstract": "High-level languages (Matlab, Python) are popular in neuroscience\nbecause they are flexible and accelerate development. However, for\nsimulating spiking neural networks, the cost of interpretation is a\nbottleneck. We describe a set of algorithms to simulate large spiking\nneural networks efficiently with high-level languages using vector-based\noperations. These algorithms constitute the core of Brian, a spiking\nneural network simulator written in the Python language. Vectorized\nsimulation makes it possible to combine the flexibility of high-level\nlanguages with the computational efficiency usually associated with\ncompiled languages.",
    "software": [
      "Brian"
    ]
  },
  "snn_gpu": {
    "year": 2012,
    "authors": "Brette R, Goodman DFM",
    "title": "Simulating spiking neural networks on GPU",
    "journal": "Network: Computation in Neural Systems",
    "additional": "23(4)",
    "categories": [
      "Neural simulation"
    ],
    "urls": [
      [
        "Journal",
        "http://informahealthcare.com/doi/abs/10.3109/0954898X.2012.730170"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/2rxybu315p0tclq/gpureview-preprint.pdf?dl=1"
      ]
    ],
    "abstract": "Modern graphics cards contain hundreds of cores that can be programmed\nfor intensive calculations. They are beginning to be used for spiking\nneural network simulations. The goal is to make parallel simulation of\nspiking neural networks available to a large audience, without the\nrequirements of a cluster. We review the ongoing efforts towards this\ngoal, and we outline the main difficulties."
  },
  "brian_encyclopedia": {
    "year": 2013,
    "authors": "Goodman DFM, Brette R",
    "title": "Brian Spiking Neural Network Simulator",
    "publisher": "SpringerReference",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Full text (paywall)",
        "https://doi.org/10.1007/978-1-0716-1006-0_253"
      ]
    ],
    "abstract": "Brian (http://briansimulator.org) is an open source Python package for\ndeveloping simulations of networks of spiking neurons (Goodman and\nBrette 2008, 2009). The design is aimed at minimizing users' development\n\ntime, with execution speed as secondary goal. Users specify neuron and\nsynapse models by giving their equations in standard mathematical form,\ncreate groups of neurons, and connect them via synapses. The intent is\nto make the process as flexible as possible so that researchers are not\nrestricted to using neuron and synapse models already built into the\nsimulator. The entire simulator is written in Python, using the NumPy\nand SciPy numerical and scientific computing packages. Parts of the\nsimulator can optionally be run using C++ code generated on the fly\n(Goodman 2010). Computationally, Brian uses vectorization techniques\n(Brette and Goodman 2011) so that for large numbers of neurons,\nexecution speed is of the same order of magnitude ... ",
    "book": "Encyclopedia of Computational Neuroscience",
    "book_editors": "Jaeger D, Jung R",
    "software": [
      "Brian"
    ]
  },
  "brian_scholarpedia": {
    "year": 2013,
    "authors": "Goodman DFM, Brette R",
    "title": "Brian simulator",
    "journal": "Scholarpedia",
    "additional": "8(1):10883",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Full text (HTML)",
        "http://www.scholarpedia.org/article/Brian_simulator"
      ]
    ],
    "abstract": "Brian is an open source Python package for developing simulations of\nnetworks of spiking neurons. The design is aimed at minimizing users'\ndevelopment time, with execution speed a secondary goal. Users specify\nneuron and synapse models by giving their equations in standard\nmathematical form, create groups of neurons and connect them via\nsynapses. The intent is to make the process as flexible as possible, so\nthat researchers are not restricted to using neuron and synapse models\nalready built in to the simulator. The entire simulator is written in\nPython, using the NumPy and SciPy numerical and scientific computing\npackages. Parts of the simulator can optionally be run using C++ code\ngenerated on the fly (Goodman 2010). Computationally, Brian uses\nvectorization techniques (Brette and Goodman 2011), so that for large\nnumbers of neurons, execution speed is of the same order of magnitude\nas C++ code (Goodman and Brette 2008, 2009).         ",
    "software": [
      "Brian"
    ]
  },
  "decoding_soundloc": {
    "year": 2013,
    "authors": "Goodman DFM, Benichoux V, Brette R",
    "title": "Decoding neural responses to temporal cues for sound localization",
    "journal": "eLife",
    "additional": "2013;2:e01312",
    "categories": [
      "Sound localisation",
      "Modelling",
      "Machine learning"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://elifesciences.org/content/2/e01312"
      ],
      [
        "Code on GitHub",
        "https://github.com/neural-reckoning/decoding_sound_location"
      ]
    ],
    "abstract": "The activity of sensory neural populations carries information about the\nenvironment. This may be extracted from neural activity using different\nstrategies. In the auditory brainstem, a recent theory proposes that\nsound location in the horizontal plane is decoded from the relative\nsummed activity of two populations in each hemisphere, whereas earlier\ntheories hypothesized that the location was decoded from the identity of\nthe most active cells. We tested the performance of various decoders of\nneural responses in increasingly complex acoustical situations, including\nspectrum variations, noise, and sound diffraction. We demonstrate that\nthere is insufficient information in the pooled activity of each\nhemisphere to estimate sound direction in a reliable way consistent with\nbehavior, whereas robust estimates can be obtained from neural activity\nby taking into account the heterogeneous tuning of cells. These\nestimates can still be obtained when only contralateral neural responses\nare used, consistently with unilateral lesion studies.        "
  },
  "playdoh": {
    "year": 2013,
    "authors": "Rossant C, Fontaine B, Goodman DFM",
    "title": "Playdoh: a lightweight Python package for distributed computing and optimisation",
    "journal": "Journal of Computational Science",
    "additional": "4(5):352-259",
    "doi": "10.1016/j.jocs.2011.06.002",
    "categories": [
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Journal",
        "https://doi.org/10.1016/j.jocs.2011.06.002"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/d2v1dvfu5xv1i9q/playdoh-preprint.pdf?dl=1"
      ]
    ],
    "abstract": "Parallel computing is now an essential paradigm for high performance\nscientific computing. Most existing hardware and software solutions are\nexpensive or difficult to use. We developed Playdoh, a Python library\nfor distributing computations across the free computing units available\nin a small network of multicore computers. Playdoh supports independent\nand loosely coupled parallel problems such as global optimisations,\nMonte Carlo simulations and numerical integration of partial\ndifferential equations. It is designed to be lightweight and easy to use\nand should be of interest to scientists wanting to turn their lab\ncomputers into a small cluster at no cost."
  },
  "equations": {
    "year": 2014,
    "authors": "Stimberg M, Goodman DFM, Benichoux V, Brette R",
    "title": "Equation-oriented specification of neural models for simulations",
    "journal": "Frontiers in Neuroinformatics",
    "additional": "8:6",
    "doi": "10.3389/fninf.2014.00006",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "http://www.frontiersin.org/Journal/abstract/69453"
      ]
    ],
    "abstract": "Simulating biological neuronal networks is a core method of research in\ncomputational neuroscience. A full specification of such a network model\nincludes a description of the dynamics and state changes of neurons and\nsynapses, as well as the synaptic connectivity patterns and the initial\nvalues of all parameters. A standard approach in neuronal modeling\nsoftware is to build network models based on a library of pre-defined\ncomponents and mechanisms; if a model component does not yet exist, it\nhas to be defined in a special-purpose or general low-level language and\npotentially be compiled and linked with the simulator. Here we propose\nan alternative approach that allows flexible definition of models by\nwriting textual descriptions based on mathematical notation. We\ndemonstrate that this approach allows the definition of a wide range of\nmodels with minimal syntax. Furthermore, such explicit model\ndescriptions allow the generation of executable code for various target\nlanguages and devices, since the description is not tied to an\nimplementation. Finally, this approach also has advantages for\nreadability and reproducibility, because the model description is fully\nexplicit, and because it can be automatically parsed and transformed\ninto formatted descriptions. The presented approach has been implemented\nin the Brian2 simulator.        ",
    "software": [
      "Brian"
    ]
  },
  "mkk": {
    "selected": false,
    "year": 2014,
    "authors": "Kadir SN, Goodman DFM, Harris KD",
    "title": "High-dimensional cluster analysis with the masked EM algorithm",
    "journal": "Neural Computation",
    "additional": "26:11",
    "doi": "10.1162/NECO_a_00661",
    "categories": [
      "Spike sorting",
      "Machine learning"
    ],
    "urls": [
      [
        "Journal",
        "http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00661"
      ],
      [
        "PDF",
        "http://www.mitpressjournals.org/doi/pdf/10.1162/NECO_a_00661"
      ],
      [
        "Preprint",
        "http://arxiv.org/abs/1309.2848"
      ],
      [
        "PDF (preprint)",
        "https://arxiv.org/pdf/1309.2848"
      ]
    ],
    "abstract": "Cluster analysis faces two problems in high dimensions: the \"curse of\ndimensionality\" that can lead to overfitting and poor generalization\nperformance and the sheer time taken for conventional algorithms to\nprocess large amounts of high-dimensional data. We describe a solution\nto these problems, designed for the application of spike sorting for\nnext-generation, high-channel-count neural probes. In this problem, only\na small subset of features provides information about the cluster\nmembership of any one data vector, but this informative feature subset\nis not the same for all data points, rendering classical feature\nselection ineffective. We introduce a \"masked EM\" algorithm that allows\naccurate and time-efficient clustering of up to millions of points in\nthousands of dimensions. We demonstrate its applicability to synthetic\ndata and to real-world high-channel-count spike sorting data.        ",
    "software": [
      "klusta"
    ]
  },
  "downstream_cns2015": {
    "year": 2015,
    "authors": "Goodman DFM, de Cheveigné A, Winter IM, Lorenzi C",
    "title": "Downstream changes in firing regularity following damage to the early auditory system",
    "conference": "Computational Neuroscience",
    "additional": "",
    "doi": "10.1186/1471-2202-16-S1-O11",
    "additional_detail": "24th Annual Computational Neuroscience Meeting: CNS*2015",
    "categories": [
      "Auditory",
      "Modelling"
    ],
    "urls": [
      [
        "Conference",
        "https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-16-S1-O11"
      ]
    ],
    "abstract": "<p>We demonstrate how an abstract mathematical model that approximates a wide range of more \ndetailed models can be used to make predictions about hearing loss-related changes in neural behaviour.</p> \n\n<p>One consequence of neurosensory hearing loss (noise-induced and aging-related) is a reduced ability to \nunderstand speech, particularly in noisy environments, and sometimes beyond what would be predicted from \nreduced audibility. Indeed, this type of speech deficit can occur in listeners with near-normal hearing \nthresholds [1]. A promising avenue of investigation to explain this comes from experimental results in mice \nshowing that there can be a permanent loss of auditory nerve fibres (ANFs) following \"temporary\" \nnoise-induced hearing loss (i.e. when thresholds return to normal after a few weeks) [2]. The downstream \nconsequences of this loss of fibres has not yet been systematically investigated (although see [3]). We \npredict, using a theoretical analysis that applies to a wide range of neural models, that the regularity of \nthe spike trains of many neurons in the cochlear nucleus (the next structure after the auditory nerve) will \ndecrease following a reduction in the number of input cells. </p>\n\n<p>We present a mathematical analysis of the stationary behaviour of \"chopper\" cells in the ventral cochlear \nnucleus, approximating them by a stochastic process that is entirely characterised by its mean, \nstandard deviation and time constants. Furthermore, these constants can be straightforwardly related to \nphysiologically significant parameters including the number of inputs and their average firing rates. From \nthis approximation, we can compute the regularity of the chopper cell spike trains measured as the \ncoefficient of variation of their interspike intervals (CV). </p>\n\n<p>One simple prediction of this model is that when the intensity of a stimulus changes, leading to a change in \nthe average firing rate of the ANF inputs, there will be a corresponding change in the regularity of the \nchopper cell spike train. This prediction poses problems for the widely used scheme for classifying chopper \ncells as sustained or transient based on their ongoing CVs as it implies that the classification could be \nlevel-dependent. We present a re-analysis of an existing experimental data set that demonstrates that ongoing \nCV is indeed level-dependent in the majority of chopper cells, and that in some cells (>7%) this leads to a \nlevel-dependence in their classification. </p>\n\n<p>Assuming a homeostatic regulation of long term firing rates, a loss of ANFs will lead to an increase in the \nstandard deviation of the stochastic process and a consequent increase in the CV of the chopper cell. Some \nchoppers that were previously classified as sustained will become transient, a substantial change in their \nbehaviour that is highly likely to disrupt auditory processing. While the function of chopper cells is still \ndebated, one suggested role is in the coding of temporal envelope [4], which is widely agreed to be essential \nfor understanding speech. Loss of ANFs could therefore lead to a disruption of the processing of temporal \nenvelope, and consequently degrade speech intelligibility. We briefly conclude by discussing the challenges \nof testing this hypothesis experimentally.</p>",
    "software": [
      "Brian"
    ]
  },
  "model_initiative_asa2016": {
    "year": 2016,
    "authors": "Dietz M, Marquardt T, Majdak P, Stern RM, Hartmann WM, Goodman DF, Ewert SD",
    "title": "A framework for auditory model comparability and applicability",
    "conference": "Acoustical Society of America",
    "additional": "",
    "doi": "10.1121/1.4970386",
    "additional_detail": "Acoustical Society of America meeting 2016",
    "categories": [
      "Auditory",
      "Modelling",
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Conference",
        "https://asa.scitation.org/doi/abs/10.1121/1.4970386"
      ]
    ],
    "abstract": "Many computational models of the auditory system exist, most of which can predict a variety of psychoacoustical, physiological, or other experimental data. However, it is often challenging to apply existing third party models to own experimental paradigms, even if the model code is available. It will be demonstrated that model applicability is increased by providing a framework where the model acts as artificial observer performing exactly the same task as the subject (e.g., adaptive staircase procedure). A possible separation of the actual auditory processing of the model from the decision making stage will be discussed, which allows for testing the auditory processing of one model in a variety of experimental paradigms. The framework will consist of a citable data repository providing the required data for the models as well as toolboxes implementing both the auditory models and a variety of experimental paradigms. The model framework will be demonstrated with exemplary binaural models applied to the three most common binaural psychoacoustic paradigms: just noticeable difference (e.g., interaural time difference), tone in noise detection (e.g., binaural masking level difference), and absolute judgment (e.g., sound source localization). Further development of the framework will be discussed.",
    "software": [
      "auditory_model_initiative"
    ]
  },
  "spikesorting": {
    "year": 2016,
    "authors": "Rossant C, Kadir SN, Goodman DFM, Schulman J, Hunter MLD, Saleem AB, Grosmark A, Belluscio M, Denfield GH, Ecker AS, Tolias AS, Solomon S, Buzsáki G, Carandini M, Harris KD",
    "title": "Spike sorting for large, dense electrode arrays",
    "journal": "Nature Neuroscience",
    "doi": "10.1038/nn.4268",
    "categories": [
      "Spike sorting",
      "Machine learning"
    ],
    "urls": [
      [
        "Journal",
        "http://dx.doi.org/10.1038/nn.4268"
      ],
      [
        "PDF",
        "http://www.nature.com/neuro/journal/vaop/ncurrent/pdf/nn.4268.pdf"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/emnup6d0aoyjccs/spikesorting-preprint.pdf?dl=1"
      ]
    ],
    "abstract": "Developments in microfabrication technology have enabled the production of\nneural electrode arrays with hundreds of closely spaced recording sites, and\nelectrodes with thousands of sites are under development. These probes in\nprinciple allow the simultaneous recording of very large numbers of neurons.\nHowever, use of this technology requires the development of techniques for\ndecoding the spike times of the recorded neurons from the raw data captured\nfrom the probes. Here we present a set of tools to solve this problem,\nimplemented in a suite of practical, user-friendly, open-source software.\nWe validate these methods on data from the cortex, hippocampus and thalamus\nof rat, mouse, macaque and marmoset, demonstrating error rates as low as 5%.",
    "software": [
      "klusta"
    ]
  },
  "hypothesis_driven_asa2017": {
    "year": 2017,
    "authors": "Goodman DF",
    "title": "On the use of hypothesis-driven reduced models in auditory neuroscience",
    "conference": "Acoustical Society of America",
    "additional": "",
    "doi": "10.1121/1.4987594",
    "additional_detail": "Acoustical Society of America meeting 2017",
    "categories": [
      "Auditory",
      "Modelling"
    ],
    "urls": [
      [
        "Conference",
        "https://asa.scitation.org/doi/abs/10.1121/1.4987594"
      ]
    ],
    "abstract": "There are a number of detailed models of auditory neurons that are able to reproduce a wide range of phenomena. However, using these models to test hypotheses can be challenging, as they have many parameters and complex interacting subsystems. This makes it difficult to investigate the function of a mechanism by varying just one parameter in isolation, or to assess the robustness of a model by systematically varying many parameters. In some cases, by limiting the scope of a model to testing a specific hypothesis using a particular set of stimuli, it is possible to create a reduced mathematical model with relatively few, independent parameters. This has considerable advantages with respect to the problems above. In particular, if a certain behavior is robust and does not depend on finely tuned parameters, then different implementations are more likely to produce the same results—a key property for reproducible research. In addition, the code for these models is typically simpler and therefore more readable, and can often run faster, enabling us to carry out systematic parameter exploration. I will illustrate these points with a reduced model of chopper cells in the ventral cochlear nucleus."
  },
  "model_initiative_asa2017": {
    "year": 2017,
    "authors": "Dietz M, Marquardt T, Majdak P, Stern RM, Hartmann WM, Goodman DF, Ewert SD",
    "title": "An initiative for testability and comparability of binaural models",
    "conference": "Acoustical Society of America",
    "additional": "",
    "doi": "10.1121/1.4987810",
    "additional_detail": "Acoustical Society of America meeting 2017",
    "categories": [
      "Auditory",
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Conference",
        "https://asa.scitation.org/doi/abs/10.1121/1.4987810"
      ]
    ],
    "abstract": "A framework aimed at improving the testability and comparability of binaural models will be presented. The framework consists of two key elements: (1) a repository of testing software that evaluates the models against published data and (2) a model repository. While the framework is also intended for physiological data, the planned initial contribution will be psychoacoustical data together with their psychoacoustical testing protocols, as well as existing binaural models from available auditory toolboxes. Researchers will be invited to provide their established as well as newly developed models in whatever programming language they prefer, given the models are compatibility with the proposed interface to the testing software. This entails that the models act as artificial observers, testable with exactly the same procedure as the human subjects. A simple communication protocol based on wav and txt-files is proposed because these are supported by every programming environment, and are able connect models and testing software of any programming language. Examples will illustrate the principle of testing models with unaltered signal processing stages on various seminal data sets such as tone detection in so-called double-delayed masking noise, or lateralization of ¾-period delayed noise and sounds with temporally asymmetric envelopes.",
    "software": [
      "auditory_model_initiative"
    ]
  },
  "roles_inhibition_adaptation_spatial_hearing": {
    "year": 2017,
    "authors": "Lestang JH, Goodman DF",
    "title": "The roles of inhibition and adaptation for spatial hearing in difficult listening conditions",
    "conference": "Acoustical Society of America",
    "additional": "",
    "doi": "10.1121/1.4987838 ",
    "additional_detail": "Acoustical Society of America meeting 2017",
    "categories": [
      "Auditory",
      "Modelling"
    ],
    "urls": [
      [
        "Conference",
        "https://asa.scitation.org/doi/abs/10.1121/1.4987838"
      ]
    ],
    "abstract": "The computation of binaural cues such as the Interaural Time Difference (ITD) and Interaural Level Difference (ILD) by the auditory system is known to play an important role in spatial hearing. It is not yet understood how such computations are performed in realistic acoustic environments where noise and reverberations are present. It has been hypothesized that robust sound localization is achieved through the extraction of the ITD information in the rising part of amplitude modulated (AM) sounds. Dietz et al. (2013) tested this hypothesis using psychoacoustics and MEG experiments. They presented AM sounds with ITDs varying during the course of one AM cycle. Their results showed that participants preferentially extracted the ITD information in the rising portion of the AM cycle. We designed a computational model of the auditory pathway to investigate the neural mechanisms involved in this process. Two mechanisms were tested. The first one corresponds to the adaptation in the auditory nerve fibers. The second mechanism occurs after coincidence detection and involves a winner-take-all network of ITD sensitive neurons. Both mechanisms qualitatively accounted for the data, consequently we suggest further experiments based on similar stimuli to distinguish between the two mechanisms. Dietz et al. (2013), “Emphasis of spatial cues in the temporal fine structure during the rising segments of amplitude-modulated sounds,” Proc. Natl. Acad. Sci. 110(37), 15151-15156.",
    "software": [
      "Brian"
    ]
  },
  "codegen_review": {
    "year": 2018,
    "authors": "Blundell I, Brette R, Cleland TA, Close TG, Coca D, Davison AP, Diaz-Pier S, Musoles CF, Gleeson P, Goodman DFM, Hines M, Hopkins MW, Kumbhar P, Lester DR, Marin B, Morrison A, Müller E, Nowotny T, Peyser A, Plotnikov D, Richmond P, Rowley A, Rumpe B, Stimberg M, Stokes AB, Tomkins A, Trensch G, Woodman M, Eppler JM",
    "title": "Code Generation in Computational Neuroscience: A Review of Tools and Techniques",
    "journal": "Frontiers in Neuroinformatics",
    "doi": "10.3389/fninf.2018.00068",
    "categories": [
      "Neuroinformatics",
      "Neural simulation",
      "Brian"
    ],
    "urls": [
      [
        "Journal",
        "https://www.frontiersin.org/articles/10.3389/fninf.2018.00068/full"
      ],
      [
        "PDF",
        "https://www.frontiersin.org/articles/10.3389/fninf.2018.00068/pdf"
      ]
    ],
    "abstract": "Advances in experimental techniques and computational power allowing researchers to gather anatomical and\nelectrophysiological data at unprecedented levels of detail have fostered the development of increasingly complex\nmodels in computational neuroscience. Large-scale, biophysically detailed cell models pose a particular set of\ncomputational challenges, and this has led to the development of a number of domain-specific simulators. At the\nother level of detail, the ever growing variety of point neuron models increases the implementation barrier even\nfor those based on the relatively simple integrate-and-fire neuron model. Independently of the model complexity,\nall modeling methods crucially depend on an efficient and accurate transformation of mathematical model\ndescriptions into efficiently executable code. Neuroscientists usually publish model descriptions in terms of the\nmathematical equations underlying them. However, actually simulating them requires they be translated into code.\nThis can cause problems because errors may be introduced if this process is carried out by hand, and code written\nby neuroscientists may not be very computationally efficient. Furthermore, the translated code might be generated\nfor different hardware platforms, operating system variants or even written in different languages and thus\ncannot easily be combined or even compared. Two main approaches to addressing this issues have been followed. The\nfirst is to limit users to a fixed set of optimized models, which limits flexibility. The second is to allow\nmodel definitions in a high level interpreted language, although this may limit performance. Recently, a third\napproach has become increasingly popular: using code generation to automatically translate high level\ndescriptions into efficient low level code to combine the best of previous approaches. This approach also greatly\nenriches efforts to standardize simulator-independent model description languages. In the past few years, a\nnumber of code generation pipelines have been developed in the computational neuroscience community, which differ\nconsiderably in aim, scope and functionality. This article provides an overview of existing pipelines currently\nused within the community and contrasts their capabilities and the technologies and concepts behind them.",
    "software": [
      "Brian"
    ]
  },
  "confluent_hierarchical_gd2018": {
    "year": 2018,
    "authors": "Zheng JX, Pawar S, Goodman DFM",
    "title": "Confluent* Drawings by Hierarchical Clustering",
    "conference": "Graph Drawing and Network Visualization",
    "additional": "",
    "doi": "10.1007/978-3-030-04414-5",
    "additional_detail": "26th International Symposium, GD 2018, Barcelona, Spain, September 26-28, 2018, Proceedings",
    "categories": [
      "Visualisation",
      "Machine learning"
    ],
    "urls": [
      [
        "Proceedings",
        "https://link.springer.com/book/10.1007/978-3-030-04414-5"
      ],
      [
        "Proceedings PDF (see p. 640)",
        "https://link.springer.com/content/pdf/10.1007%2F978-3-030-04414-5.pdf"
      ]
    ],
    "abstract": "Recently an edge bundling technique known as confluent\u0002* drawing was applied to general graphs by Bach et al. (2017) by leveraging power graph decomposition (a form of edge compression that groups similar vertices together, merging edges shared among group members). We explore the technique further by demonstrating the equivalence between confluent drawing and the hierarchical edge bundling of Holten (2006), thereby opening the door for existing hierarchical clustering algorithms to be used instead of power graphs to produce confluent drawings for general graphs. We investigate various popular hierarchical clustering methods, and present a qualitative experimental comparison between them. We also introduce a new distance measure for agglomerative clustering that outperforms previous measures, and make recommendations for using the method in practice."
  },
  "framework_comparing_binaural_models": {
    "year": 2018,
    "authors": "Dietz M, Lestang J-H, Majdak P, Stern RM, Marquardt T, Ewert SD, Hartmann WH, Goodman DFM",
    "title": "A framework for testing and comparing binaural models",
    "journal": "Hearing Research",
    "doi": "10.1016/j.heares.2017.11.010",
    "categories": [
      "Auditory",
      "Neuroinformatics",
      "Sound localisation"
    ],
    "urls": [
      [
        "Journal",
        "https://doi.org/10.1016/j.heares.2017.11.010"
      ],
      [
        "PDF (preprint)",
        "https://www.dropbox.com/s/v64783umlei7448/framework-for-testing-and-comparing-binaural-models.pdf?dl=1"
      ],
      [
        "Code (GitHub)",
        "https://github.com/model-initiative/model_initiative"
      ]
    ],
    "abstract": "Auditory research has a rich history of combining experimental evidence with computational simulations of\nauditory processing in order to deepen our theoretical understanding of how sound is processed in the ears and in\nthe brain. Despite significant progress in the amount of detail and breadth covered by auditory models, for many\ncomponents of the auditory pathway there are still different model approaches that are often not equivalent but\nrather in conflict with each other. Similarly, some experimental studies yield conflicting results which has led\nto controversies. This can be best resolved by a systematic comparison of multiple experimental data sets and\nmodel approaches. Binaural processing is a prominent example of how the development of quantitative theories can\nadvance our understanding of the phenomena, but there remain several unresolved questions for which competing\nmodel approaches exist. This article discusses a number of current unresolved or disputed issues in binaural\nmodeling, as well as some of the significant challenges in comparing binaural models with each other and with the\nexperimental data. We introduce an auditory model framework, which we believe can become a useful infrastructure\nfor resolving some of the current controversies. It operates models over the same paradigms that are used\nexperimentally. The core of the proposed framework is an interface that connects three components irrespective of\ntheir underlying programming language: The experiment software, an auditory pathway model, and task-dependent\ndecision stages called artificial observers that provide the same output format as the test subject.",
    "software": [
      "auditory_model_initiative"
    ]
  },
  "graph_drawing_wcr": {
    "year": 2018,
    "authors": "Zheng JX, Pawar S, Goodman DFM",
    "title": "Graph Drawing by Stochastic Gradient Descent",
    "journal": " IEEE Transactions on Visualization and Computer Graphics",
    "doi": "10.1109/TVCG.2018.2859997",
    "categories": [
      "Visualisation",
      "Machine learning",
      "Software"
    ],
    "urls": [
      [
        "Journal",
        "https://ieeexplore.ieee.org/document/8419285"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/1710.04626"
      ],
      [
        "PDF (preprint)",
        "https://arxiv.org/pdf/1710.04626"
      ],
      [
        "Code (GitHub)",
        "https://github.com/jxz12/s_gd2"
      ]
    ],
    "abstract": "A popular method of force-directed graph drawing is multidimensional scaling using graph-theoretic\ndistances as input. We present an algorithm to minimize its energy function, known as stress, by\nusing stochastic gradient descent (SGD) to move a single pair of vertices at a time. Our results\nshow that SGD can reach lower stress levels faster and more consistently than majorization, without\nneeding help from a good initialization. We then present various real-world applications to show\nhow the unique properties of SGD make it easier to produce constrained layouts than previous\napproaches. We also show how SGD can be directly applied within the sparse stress approximation of\nOrtmann et al. [1], making the algorithm scalable up to large graphs.",
    "software": [
      "sgd2"
    ]
  },
  "re_stdp_repeating_patterns": {
    "year": 2018,
    "authors": "Hathway P, Goodman DFM",
    "title": "[Re] Spike Timing Dependent Plasticity Finds the Start of Repeating Patterns in Continuous Spike Trains",
    "journal": "ReScience",
    "doi": "10.5281/zenodo.1327348",
    "categories": [
      "Modelling",
      "Learning",
      "Plasticity",
      "Spiking"
    ],
    "urls": [
      [
        "PDF",
        "https://github.com/ReScience-Archives/Hathway-Goodman-2018/raw/master/article/Hathway-Goodman-2018.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/pamelahathway/ReScience-submission/tree/Hathway-Goodman/code"
      ],
      [
        "Code (Zenodo)",
        "https://doi.org/10.5281/zenodo.1327348"
      ],
      [
        "Review",
        "https://github.com/ReScience/ReScience-submission/pull/51"
      ]
    ],
    "abstract": "This article is a replication of\n<a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0001377\">Masquelier et al. (2008)\n\"Spike Timing Dependent Plasticity Finds the Start of Repeating Patterns in Continuous Spike Trains\"</a>.",
    "software": [
      "Brian"
    ]
  },
  "vcn_regularity": {
    "year": 2018,
    "authors": "Goodman DFM, Winter IM, Léger AC, de Cheveigné A, Lorenzi C",
    "title": "Modelling firing regularity in the ventral cochlear nucleus: mechanisms, and effects of stimulus level and synaptopathy",
    "journal": "Hearing Research",
    "doi": "10.1016/j.heares.2017.09.010",
    "categories": [
      "Auditory",
      "Spiking",
      "Modelling"
    ],
    "urls": [
      [
        "Journal",
        "https://doi.org/10.1016/j.heares.2017.09.010"
      ],
      [
        "Preprint",
        "https://www.biorxiv.org/content/early/2017/09/19/121707"
      ],
      [
        "PDF (preprint)",
        "https://www.biorxiv.org/content/early/2017/09/19/121707.full.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/neural-reckoning/vcn_regularity"
      ],
      [
        "Code (Binder, interactive)",
        "http://mybinder.org/repo/neural-reckoning/vcn_regularity"
      ]
    ],
    "abstract": "The auditory system processes temporal information at multiple scales, and disruptions to this temporal\nprocessing may lead to deficits in auditory tasks such as detecting and discriminating sounds in a noisy\nenvironment. Here, a modelling approach is used to study the temporal regularity of firing by chopper cells in\nthe ventral cochlear nucleus, in both the normal and impaired auditory system. Chopper cells, which have a\nstrikingly regular firing response, divide into two classes, sustained and transient, based on the time course of\nthis regularity. Several hypotheses have been proposed to explain the behaviour of chopper cells, and the\ndifference between sustained and transient cells in particular. However, there is no conclusive evidence so far.\nHere, a reduced mathematical model is developed and used to compare and test a wide range of hypotheses with a\nlimited number of parameters. Simulation results show a continuum of cell types and behaviours: chopper-like\nbehaviour arises for a wide range of parameters, suggesting that multiple mechanisms may underlie this behaviour.\nThe model accounts for systematic trends in regularity as a function of stimulus level that have previously only\nbeen reported anecdotally. Finally, the model is used to predict the effects of a reduction in the number of\nauditory nerve fibres (deafferentation due to, for example, cochlear synaptopathy). An interactive\nversion of this paper in which all the model parameters can be changed is available online.",
    "software": [
      "Brian"
    ]
  },
  "vr_mobile_binaural": {
    "year": 2018,
    "authors": "Chungeun K, Steadman M, Lestang JH, Goodman DFM, Picinali L",
    "title": "A VR-Based Mobile Platform for Training to Non-Individualized Binaural 3D Audio",
    "conference": "Audio Engineering Society ",
    "additional": "",
    "doi": "",
    "additional_detail": "AES Convention: 144 (May 2018)",
    "categories": [
      "Sound localisation",
      "Virtual reality"
    ],
    "urls": [
      [
        "Conference",
        "http://www.aes.org/e-lib/browse.cfm?elib=19406"
      ]
    ],
    "abstract": "Delivery of immersive 3D audio with arbitrarily-positioned sound sources over headphones often requires processing of individual source signals through a set of Head-Related Transfer Functions (HRTFs), the direction-dependent filters that describe the propagation of sound in an anechoic environment from the source to the listener's ears. The individual morphological differences and the impracticality of HRTF measurement make it difficult to deliver completely individualized 3D audio in this manner, and instead lead to the use of previously-measured non-individual sets of HRTFs. In this study a VR-based mobile sound localization training prototype system is introduced that uses HRTF sets for audio. It consists of a mobile phone as a head-mounted device, a hand-held Bluetooth controller, and a network-enabled PC with a USB audio interface and a pair of headphones. The virtual environment was developed on the mobile phone such that the user can listen-to/navigate-in an acoustically neutral scene and locate invisible target sound sources presented at random directions using non-individualized HRTFs in repetitive sessions. Various training paradigms can be designed with this system, with performance-related feedback provided according to the user's localization accuracy, including visual indication of the target location, and some aspects of a typical first-person shooting game, such as enemies, scoring, and level advancement. An experiment was conducted using this system in which 11 subjects went through multiple training sessions, using non-individualized HRTF sets. The localization performance evaluations showed reduction of overall localization angle error over repeated training sessions, reflecting lower front-back confusion rates."
  },
  "astrocytes": {
    "year": 2019,
    "authors": "Stimberg M, Goodman DFM, Brette R, De Pittà M",
    "title": "Modeling neuron-glia interactions with the Brian 2 simulator",
    "publisher": "Springer",
    "categories": [
      "Brian",
      "Modelling"
    ],
    "urls": [
      [
        "Book",
        "https://link.springer.com/book/10.1007/978-3-030-00817-8"
      ],
      [
        "Chapter",
        "https://link.springer.com/chapter/10.1007/978-3-030-00817-8_18"
      ],
      [
        "Preprint",
        "https://www.biorxiv.org/content/early/2017/10/05/198366"
      ],
      [
        "PDF (preprint)",
        "https://www.biorxiv.org/content/early/2017/10/05/198366.full.pdf"
      ]
    ],
    "abstract": "Despite compelling evidence that glial cells could crucially regulate neural network activity, the vast majority\nof available neural simulators ignores the possible contribution of glia to neuronal physiology. Here, we show\nhow to model glial physiology and neuron-glia interactions in the Brian 2 simulator. Brian 2 offers facilities to\nexplicitly describe any model in mathematical terms with limited and simple simulator-specific syntax,\nautomatically generating high-performance code from the user-provided descriptions. The flexibility of this\napproach allows us to model not only networks of neurons, but also individual glial cells, electrical coupling of\nglial cells, and the interaction between glial cells and synapses. We therefore conclude that Brian 2 provides an\nideal platform to efficiently simulate glial physiology, and specifically, the influence of astrocytes on neural\nactivity.",
    "book_chapter": true,
    "book": "Computational Glioscience",
    "book_editors": "De Pittà M, Berry H",
    "software": [
      "Brian"
    ]
  },
  "attention_ccn2019": {
    "year": 2019,
    "authors": "Chu Y, Goodman DFM",
    "title": "An Inference Network Model for Goal-directed Attentional Selection",
    "conference": "Cognitive Computational Neuroscience",
    "additional": "",
    "doi": "10.32470/CCN.2019.1431-0",
    "additional_detail": "2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany",
    "categories": [
      "Machine learning",
      "Modelling",
      "Visual"
    ],
    "urls": [
      [
        "Abstract",
        "https://ccneuro.org/2019/Papers/ViewPapers.asp?PaperNum=1431"
      ],
      [
        "PDF",
        "https://ccneuro.org/2019/showDoc.php?s=W&pn=1431"
      ]
    ],
    "abstract": "\"Listen to the cello in this symphony!\" How can we direct selective attention according to different goals, even\nin distracting environments which we haven't experienced before? It is an essential cognitive ability of the\nbrain, but remains challenging for machines. We developed a computational model that can identify individual\ndigits in images containing multiple overlapping digits, without ever having seen overlapping digits during\ntraining. The goal-driven attentional selection is modelled as inferring the posterior distribution of latent\nvariables (the attended target) in a generative model, conditioned on both sensory input and different semantic\ngoals. A neural network model has been build to efficiently carry out the the inference process by predicting the\nmost likely results, instead of using classic per-sample based iterative optimization methods which may not\nnaturally map onto neural structures. Our model also help to understand how top-down and bottom-up attention are\ncombined during perception in the brain."
  },
  "auditory_anchors": {
    "year": 2019,
    "authors": "Engel I, Goodman DFM, Picinali L",
    "title": "The Effect of Auditory Anchors on Sound Localization: A Preliminary Study",
    "conference": "Immersive and Interactive Audio ",
    "additional": "",
    "doi": "",
    "additional_detail": "2019 AES International Conference on Immersive and Interactive Audio (March 2019)",
    "categories": [
      "Sound localisation",
      "Virtual reality"
    ],
    "urls": [
      [
        "Conference",
        "http://www.aes.org/e-lib/browse.cfm?elib=20388"
      ]
    ],
    "abstract": "Traditional sound localization studies are often performed in anechoic chambers and in complete darkness. In our daily life, however, we are exposed to rich auditory scenes with multiple sound sources and complementary visual information. Although it is understood that the presence of maskers hinders auditory spatial awareness, it is not known whether competing sound sources can provide spatial information that helps in localizing a target stimulus. In this study, we explore the effect of presenting controlled auditory scenes with different amounts of visual and spatial cues during a sound localization task. A novel, gamified localization task is also presented. Preliminary results suggest that subjects who are exposed to audio-visual anchors show faster improvements than those who are not."
  },
  "brian2": {
    "selected": true,
    "year": 2019,
    "authors": "Stimberg M, Brette R, Goodman DFM",
    "title": "Brian 2, an intuitive and efficient neural simulator",
    "journal": "eLife",
    "additional": "8:e47314",
    "doi": "10.7554/eLife.47314",
    "categories": [
      "Brian"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://elifesciences.org/articles/47314"
      ],
      [
        "Code",
        "https://github.com/brian-team/brian2"
      ],
      [
        "Documentation",
        "https://brian2.readthedocs.io/en/stable/"
      ],
      [
        "Examples code",
        "https://github.com/brian-team/brian2_paper_examples"
      ],
      [
        "Interactive examples",
        "https://mybinder.org/v2/gh/brian-team/brian2_paper_examples/master?filepath=index.ipynb"
      ],
      [
        "Website",
        "http://briansimulator.org/"
      ]
    ],
    "abstract": "Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can\nfeature novel dynamical equations, their interactions with the environment, and experimental protocols. To\npreserve high performance when defining new models, most simulators offer two options: low-level programming or\ndescription languages. The first option requires expertise, is prone to errors, and is problematic for\nreproducibility. The second option cannot describe all aspects of a computational experiment, such as the\npotentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation.\nScientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient\nlow-level code that can run interleaved with their code. We illustrate this with several challenging examples: a\nplastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron\nmodel, and an auditory model with real-time input.",
    "software": [
      "Brian"
    ]
  },
  "canonical_ambb": {
    "year": "Preprints",
    "last_updated": "29-06-2019",
    "authors": "Lestang J-H, Goodman DFM",
    "title": "General neural mechanisms can account for rising slope preference in localization of ambiguous sounds",
    "categories": [
      "Auditory",
      "Sound localisation",
      "Modelling"
    ],
    "urls": [
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/687178v2"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/687178v2.full.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/neural-reckoning/simple_ambb_modelling"
      ],
      [
        "Live code (Binder)",
        "https://mybinder.org/v2/gh/neural-reckoning/simple_ambb_modelling/master?filepath=index.ipynb"
      ]
    ],
    "abstract": "Sound localization in reverberant environments is a difficult task that human listeners perform effortlessly.\nMany neural mechanisms have been proposed to account for this behavior. Generally they rely on emphasizing localization\ninformation at the onset of the incoming sound while discarding localization cues that arrive later. We modelled several\nof these mechanisms using neural circuits commonly found in the brain and tested their performance in the context of\nexperiments showing that, in the dominant frequency region for sound localisation, we have a preference for auditory cues\narriving during the rising slope of the sound energy (Dietz et al., 2013). We found that both single cell mechanisms (onset\nand adaptation) and population mechanisms (lateral inhibition) were easily able to reproduce the results across a very wide\nrange of parameter settings. This suggests that sound localization in reverberant environments may not require specialised\nmechanisms specific to perform that task, but could instead rely on common neural circuits in the brain. This would allow\nfor the possibility of individual differences in learnt strategies or neuronal parameters. This research is fully\nreproducible, and we made our code available to edit and run online via interactive live notebooks.",
    "software": [
      "Brian"
    ]
  },
  "data_driven_auditory_ccn2019": {
    "year": 2019,
    "authors": "Weerts L, Clopath C, Goodman DFM",
    "title": " A Unifying Framework for Neuro-Inspired, Data-Driven Detection of Low-Level Auditory Features",
    "conference": "Cognitive Computational Neuroscience",
    "additional": "",
    "doi": "10.32470/CCN.2019.1245-0",
    "additional_detail": "2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany",
    "categories": [
      "Machine learning",
      "Auditory",
      "Modelling"
    ],
    "urls": [
      [
        "Abstract",
        "https://ccneuro.org/2019/Papers/ViewPapers.asp?PaperNum=1245"
      ],
      [
        "PDF",
        "https://ccneuro.org/2019/showDoc.php?s=W&pn=1245"
      ]
    ],
    "abstract": "Our understanding of hearing and speech recognition rests on controlled experiments requiring simple stimuli. However, these stimuli often lack the characteristics of complex sounds such as speech. We propose an approach that combines neural modelling with machine learning to determine relevant low-level auditory features. Our approach bridges the gap between detailed neuronal models that capture specific auditory responses, and research on the statistics of real-world speech data and speech recognition. First, we introduce a feature detection model with a modest number of parameters that is compatible with auditory physiology. In order to objectively determine relevant feature detectors within the model parameter space, the model is tested in a speech classification task, using a simple classifier that approximates the information bottleneck. This framework allows us to determine the best model parameters and their neurophysiological and psychoacoustic implications. We show that our model can capture a variety of well-studied features (such as amplitude modulations and onsets) and allows us to unify concepts from different areas of hearing research. Our approach has various potential applications. Firstly, it could lead to new, testable experimental hypotheses for understanding hearing. Moreover, promising features could be directly applied as a new acoustic front-end for speech recognition systems."
  },
  "gamification_sound_localisation": {
    "year": 2019,
    "authors": "Steadman MA, Kim C, Lestang JH, Goodman DFM, Picinali L",
    "title": "Short-term effects of sound localization training in virtual reality",
    "journal": "Scientific Reports",
    "additional": "9, 18284",
    "doi": "10.1038/s41598-019-54811-w",
    "categories": [
      "Auditory",
      "Sound localisation",
      "Learning",
      "Virtual reality"
    ],
    "urls": [
      [
        "Journal",
        "https://www.nature.com/articles/s41598-019-54811-w"
      ],
      [
        "PDF (journal)",
        "https://www.nature.com/articles/s41598-019-54811-w.pdf"
      ],
      [
        "Data and code (Zenodo)",
        "https://zenodo.org/record/2594832"
      ],
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/207753v3"
      ],
      [
        "PDF (preprint)",
        "https://www.biorxiv.org/content/biorxiv/early/2019/07/10/207753.full-text.pdf"
      ]
    ],
    "abstract": "Head-related transfer functions (HRTFs) capture the direction-dependant way that sound interacts with the head and torso. In virtual audio systems, which aim to emulate these effects, non-individualized, generic HRTFs are typically used leading to an inaccurate perception of virtual sound location. Training has the potential to exploit the brain's ability to adapt to these unfamiliar cues. In this study, three virtual sound localization training paradigms were evaluated; one provided simple visual positional confirmation of sound source location, a second introduced game design elements (\"gamification\") and a final version additionally utilized head-tracking to provide listeners with experience of relative sound source motion (\"active listening\"). The results demonstrate a significant effect of training after a small number of short (12-minute) training sessions, which is retained across multiple days. Gamification alone had no significant effect on the efficacy of the training, but active listening resulted in a significantly greater improvements in localization accuracy. In general, improvements in virtual sound localization following training generalized to a second set of non-individualized HRTFs, although some HRTF-specific changes were observed in polar angle judgement for the active listening group. The implications of this on the putative mechanisms of the adaptation process are discussed."
  },
  "heterogeneity_ccn2019": {
    "year": 2019,
    "authors": "Perez-Nieves N, Leung VCH, Dragotti PL, Goodman DFM",
    "title": "Advantages of heterogeneity of parameters in spiking neural network training",
    "conference": "Cognitive Computational Neuroscience",
    "additional": "",
    "doi": "10.32470/CCN.2019.1173-0",
    "additional_detail": "2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany",
    "categories": [
      "Machine learning",
      "Spiking",
      "Modelling"
    ],
    "urls": [
      [
        "Abstract",
        "https://ccneuro.org/2019/Papers/ViewPapers.asp?PaperNum=1173"
      ],
      [
        "PDF",
        "https://ccneuro.org/2019/showDoc.php?s=W&pn=1173"
      ]
    ],
    "abstract": "It is very common in studies of the learning capabilities of spiking neural networks (SNNs) to use homogeneous neural and synaptic parameters (time constants, thresholds, etc.). Even in studies in which these parameters are distributed heterogeneously, the advantages or disadvantages of the heterogeneity have rarely been studied in depth. By contrast, in the brain, neurons and synapses are highly diverse, leading naturally to the hypothesis that this heterogeneity may be advantageous for learning. Starting from two state-of-the-art methods for training spiking neural networks (Nicola & Clopath, 2017, Shrestha & Orchard 2018), we found that adding parameter heterogeneity reduced errors when the network had to learn more complex patterns, increased robustness to hyperparameter mistuning, and reduced the number of training iterations required. We propose that neural heterogeneity may be an important principle for brains to learn robustly in real world environments with highly complex structure, and where task-specific hyperparameter tuning may be impossible. Consequently, heterogeneity may also be a good candidate design principle for artificial neural networks, to reduce the need for expensive hyperparameter tuning as well as for reducing training time. "
  },
  "lestang_thesis": {
    "year": 2019,
    "authors": "Lestang J-H",
    "title": "The role of canonical neural computations in sound localization",
    "publisher": "Imperial College London",
    "phd_thesis": true,
    "categories": [
      "Neuroscience",
      "Modelling",
      "Auditory",
      "Sound localisation"
    ],
    "urls": [
      [
        "Thesis",
        "https://spiral.imperial.ac.uk/handle/10044/1/76509"
      ]
    ],
    "abstract": "Localizing sounds is an important ability for many species. However, reverberative sounds present a significant challenge to the auditory system as later arriving reverberations may carry confounding localization cues. The \"precedence effect\" refers to a set of perceptual behaviours related to this situation. Studies investigating the precedence effect observed that the auditory system tends to focus the core of the localization process on the computation of localization cues carried by the first arriving sound. Doing so relieves the auditory system from dealing with contradictory localization cues in later arriving sounds. A recent study by Dietz et al. (2013) confirmed that human listeners use this approach to deal with dynamic localization cues. In order to provide an explanation for this finding, we first tested several auditory models on the specific task described in Dietz et al. (2013) in order to shortlist possible mechanisms capable of accounting for the early extraction of temporal binaural cues. We found that the best candidates to account for this data are single cell mechanisms, such as adaptation and onset firing, as well as inhibitory population mechanisms. To further understand how each mechanism contributes to the suppression of lagging sounds, we designed more general models capable of demonstrating the principal features of each mechanism. We tested these models thoroughly and found that all mechanisms were able to reproduce the results over a wide range of parameters. This finding suggests that mechanisms responsible for the precedence effect may not be specialized to perform this specific task but instead may be the results of more commonly found neural circuits in the brain. Finally, to facilitate comparing the performance of auditory models on psychoacoustical data, we also designed and implemented an auditory modelling framework capable of addressing many challenges existing in the field of auditory modelling.",
    "software": [
      "Brian",
      "auditory_model_initiative"
    ]
  },
  "neural_topic_modelling_ccn2019": {
    "year": 2019,
    "authors": "Hathway P, Goodman DFM",
    "title": " Neural Topic Modelling",
    "conference": "Cognitive Computational Neuroscience",
    "additional": "",
    "doi": "10.32470/CCN.2019.1382-0",
    "additional_detail": "2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany",
    "categories": [
      "Machine learning",
      "Neuroinformatics",
      "Neural data analysis"
    ],
    "urls": [
      [
        "Abstract",
        "https://ccneuro.org/2019/Papers/ViewPapers.asp?PaperNum=1382"
      ],
      [
        "PDF",
        "https://ccneuro.org/2019/showDoc.php?s=W&pn=1382"
      ]
    ],
    "abstract": "We introduce neural topic modelling - an unsupervised, scalable and interpretable neural data analysis tool which can be applied across different spatial and temporal scales. The aim is an approach that can handle the ever-increasing number of neurons recorded by high channel count multi-electrode arrays. Neural topic modelling is based on latent Dirichlet allocation, a method routinely used in text mining to find latent topics in texts. The spike trains are converted into \"neural words\" - the presence or absence of discrete events (e.g. neuron 1 has a higher firing rate than usual). Neural topic modelling results in a number of topics (probability distributions over words) which best explain the given co-occurrences of neural words over time. Applied to an electrophysiological dataset of mouse visual cortex, hippocampus and thalamus neurons, neural topic modelling groups neural words into topics which exhibit common attributes such as overlapping receptive fields or proximity on the recording electrode. It recovers these relationships despite receiving no knowledge about the cortex topography or about the spatial structure of the stimuli. Choosing neural activity patterns as neural words that are relevant to the brain makes the topics interpretable by both the brain and researchers, setting neural topic modelling apart from other machine learning approaches."
  },
  "brian2genn": {
    "year": 2020,
    "authors": "Stimberg M, Goodman DFM, Nowotny T",
    "title": "Brian2GeNN: a system for accelerating a large variety of spiking neural networks with graphics hardware",
    "journal": "Scientific Reports",
    "additional": "10, 410",
    "doi": "10.1038/s41598-019-54957-7",
    "categories": [
      "Brian",
      "Neuroinformatics",
      "Neural simulation"
    ],
    "urls": [
      [
        "Journal",
        "https://www.nature.com/articles/s41598-019-54957-7"
      ],
      [
        "Journal (PDF)",
        "https://www.nature.com/articles/s41598-019-54957-7.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/brian-team/brian2genn"
      ],
      [
        "Documentation",
        "https://brian2genn.readthedocs.io/en/stable/"
      ],
      [
        "Preprint",
        "https://www.biorxiv.org/content/early/2018/10/20/448050"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/early/2018/10/20/448050.full.pdf"
      ]
    ],
    "abstract": "\"Brian\" is a popular Python-based simulator for spiking neural networks, commonly used in computational neuroscience. GeNN is a C++-based meta-compiler for accelerating spiking neural network simulations using consumer or high performance grade graphics processing units (GPUs). Here we introduce a new software package, Brian2GeNN, that connects the two systems so that users can make use of GeNN GPU acceleration when developing their models in Brian, without requiring any technical knowledge about GPUs, C++ or GeNN. The new Brian2GeNN software uses a pipeline of code generation to translate Brian scripts into C++ code that can be used as input to GeNN, and subsequently can be run on suitable NVIDIA GPU accelerators. From the user's perspective, the entire pipeline is invoked by adding two simple lines to their Brian scripts. We have shown that using Brian2GeNN, two non-trivial models from the literature can run tens to hundreds of times faster than on CPU.",
    "software": [
      "Brian"
    ]
  },
  "elife_labs_matching": {
    "year": 2020,
    "authors": "Achakulvisut T, Ruangrong T, Acuna DE, Wyble B, Goodman D, Kording K",
    "title": "neuromatch: Algorithms to match scientists",
    "journal": "eLife Labs",
    "categories": [
      "Neuroscience",
      "Machine learning",
      "Metascience"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://elifesciences.org/labs/5ed408f4/neuromatch-algorithms-to-match-scientists"
      ],
      [
        "Neuromatch",
        "https://www.neuromatch.io/"
      ]
    ],
    "abstract": "We developed machine-learning algorithms to help connect scientists during online\nresearch conferences. ",
    "peer_reviewed": false
  },
  "elife_neuromatch": {
    "year": 2020,
    "authors": "Achakulvisut T, Ruangrong T, Bilgin I, Van Den Bossche S, Wyble B, Goodman DFM, Kording KP",
    "title": "Point of View: Improving on legacy conferences by moving online",
    "journal": "eLife",
    "additional": "9:e57892",
    "doi": "10.7554/eLife.57892",
    "categories": [
      "Neuroscience",
      "Machine learning",
      "Metascience"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://elifesciences.org/articles/57892"
      ],
      [
        "Neuromatch",
        "https://www.neuromatch.io/"
      ]
    ],
    "abstract": "Scientific conferences and meetings have an important role in research, but they\nalso suffer from a number of disadvantages: in particular, they can have a massive\ncarbon footprint, they are time-consuming, and the high costs involved in attending can\nexclude many potential participants. The COVID-19 pandemic has led to the cancellation\nof many conferences, forcing the scientific community to explore online alternatives.\nHere, we report on our experiences of organizing an online neuroscience conference,\nneuromatch, that attracted some 3000 participants and featured two days of talks,\ndebates, panel discussions, and one-on-one meetings facilitated by a matching algorithm.\nBy offering most of the benefits of traditional conferences, several clear advantages,\nand with fewer of the downsides, we feel that online conferences have the potential to\nreplace many legacy conferences.",
    "peer_reviewed": false
  },
  "hathway_thesis": {
    "year": 2020,
    "authors": "Hathway P",
    "title": "Biologically-inspired machine learning approaches to large-scale neural data analysis",
    "publisher": "Imperial College London",
    "phd_thesis": true,
    "categories": [
      "Machine learning",
      "Visualisation",
      "Software",
      "Neuroscience",
      "Neuroinformatics",
      "Neural data analysis",
      "Learning",
      "Spiking",
      "Plasticity"
    ],
    "urls": [
      [
        "Thesis",
        "https://spiral.imperial.ac.uk/handle/10044/1/86622"
      ]
    ],
    "abstract": "<p>Recent progress in recording techniques now allows researchers to record\nfrom hundreds and even thousands of neurons simultaneously. New, scalable\nmethods need to be developed to handle such large data sets. Ideally\nthese methods should not only analyse the multi-dimensional data, but also\nprovide results which could be interpreted by the brain.</p>\n\n<p>One key problem in neuronal data analysis is to identify neuronal assemblies\ni.e. groups of neurons displaying coordinated neuronal activity. Currently\navailable methods either search for assemblies whose neurons participate\nin repeated spike sequences or search for assemblies whose neurons display\nsimilar firing rate modulations. In this thesis, I present two approaches\nto the search for neuronal assemblies.</p>\n\n<p>I investigate whether a spiking neural network equipped with biologically\nplausible synaptic learning rules can provide a biologically interpretable way\nof finding repeating spike patterns in neuronal data. Due to the similarities\nbetween spiking neural networks to how brains function, this might be very\nclose to how the brain itself detects such repeated activity.</p>\n\n<p>Furthermore, I present neural topic modelling – a new data analysis method\nfor large neuronal data sets. Based on a machine learning method from text\nmining, neural topic modelling is scalable and produces interpretable results.\nBy including multiple features of neuronal spike trains and even other\ndata types such as local field potentials into the analysis, I can expand the\ndefinition of neuronal assemblies to any type of neuronal activity features\nwhich are co-modulated. The application of neural topic modelling to neuronal\nrecordings reveals interactions between features of neuronal activity\nwhich have previously not been identified.</p>\n\n<p>Since both approaches are biologically plausible, the results from both\nmethods can be used to generate hypotheses about how the brain processes\ninformation and may reveal hitherto unknown information processing pathways.</p>",
    "software": [
      "Brian"
    ]
  },
  "comments_on_edge_bundling": {
    "year": 2021,
    "authors": "Zheng JX, Pawar S, Goodman DFM",
    "title": "Further Towards Unambiguous Edge Bundling: Investigating Power-Confluent Drawings for Network Visualization",
    "journal": "IEEE Transactions on Visualization and Computer Graphics",
    "doi": "10.1109/TVCG.2019.2944619",
    "categories": [
      "Visualisation",
      "Software"
    ],
    "urls": [
      [
        "Journal",
        "https://ieeexplore.ieee.org/document/8852738"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/1810.09948"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/1810.09948"
      ],
      [
        "Code (GitHub)",
        "https://github.com/jxz12/pconfluent"
      ]
    ],
    "abstract": "Bach et al. [1] recently presented an algorithm for constructing confluent drawings, by leveraging power graph\ndecomposition to generate an auxiliary routing graph. We identify two problems with their method and offer a\nsingle solution to solve both. We also classify the exact type of confluent drawings that the algorithm can\nproduce as 'power-confluent', and prove that it is a subclass of the previously studied 'strict confluent'\ndrawing. A description and source code of our implementation is also provided, which additionally includes an\nimproved method for power graph construction.",
    "software": [
      "pconfluent"
    ]
  },
  "engel_thesis": {
    "year": 2021,
    "authors": "Engel I",
    "title": "Improving binaural audio techniques for augmented reality",
    "publisher": "Imperial College London",
    "phd_thesis": true,
    "categories": [
      "Machine learning",
      "Auditory",
      "Virtual reality",
      "Sound localisation"
    ],
    "urls": [
      [
        "Thesis",
        "https://spiral.imperial.ac.uk/handle/10044/1/92700"
      ],
      [
        "Thesis (PDF)",
        "https://spiral.imperial.ac.uk/bitstream/10044/1/92700/4/Engel-AM-JIE-2021-PhD-Thesis.pdf"
      ]
    ],
    "abstract": "Audio augmented reality (AAR) is defined as the extension of a real auditory environment through virtual sound sources. A successful AAR system should create the illusion that virtual sounds actually come from the user's environment, for which several technical challenges must be overcome. First, room acoustics must be simulated accurately to predict the reverberant sound field produced by the virtual source as sound wavefronts reach the user. Second, said sound field must be translated into a pair of sound pressure signals at the user's ears. Finally, this binaural signal must be delivered to the user through an acoustically transparent system without limiting their ability to hear real sources. This process should be able to adapt in real time to user movements in a computationally efficient way, considering that resources may be limited in practice and most of them will likely be allocated to graphics processing (e.g. in a pair of augmented reality glasses). This Thesis aims to improve current techniques for binaural audio rendering in AAR by exploring the trade-off between computational complexity and perceived quality. Several perception-focused studies were proposed to explore the different parts of the rendering process. First, a prototype AAR system with hear-through functionality was proposed and a pilot experiment was conducted to investigate how users could adapt to it over time. A second study assessed the effect of non-individualised equalisation on the perceived quality of binaural renderings reproduced with open-ear headphones. A third study evaluated several state-of-the-art methods for the binaural rendering of sound fields of limited resolution in the spherical harmonics (Ambisonics) domain. Finally, a fourth study assessed the perceptual effect of simplifying Ambisonics-based binaural reverberation in various ways. Even though this Thesis focuses on the AAR scenario, the findings herein may be helpful for any application that would benefit from a computationally efficient implementation of binaural audio rendering methods."
  },
  "heterogeneity": {
    "selected": true,
    "year": 2021,
    "last_updated": "4-10-2021",
    "authors": "Perez-Nieves N, Leung VCH, Dragotti PL, Goodman DFM",
    "title": "Neural heterogeneity promotes robust learning",
    "journal": "Nature Communications",
    "additional": "12, 5791",
    "doi": "10.1038/s41467-021-26022-3",
    "categories": [
      "Neuroscience",
      "Learning",
      "Visual",
      "Auditory",
      "Spiking",
      "Machine learning",
      "Modelling"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://www.nature.com/articles/s41467-021-26022-3"
      ],
      [
        "Journal (PDF)",
        "https://www.nature.com/articles/s41467-021-26022-3.pdf"
      ],
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/2020.12.18.423468v3"
      ],
      [
        "Code (GitHub)",
        "https://github.com/npvoid/neural_heterogeneity"
      ],
      [
        "Code (Zenodo)",
        "https://zenodo.org/record/5413181"
      ],
      [
        "Neurotheory talk by Dan Goodman (video)",
        "https://www.youtube.com/watch?v=V2HFqVfeTPg&feature=youtu.be"
      ],
      [
        "SNUFA 2021 talk by Nicolas Perez (video)",
        "https://youtu.be/aLNGcs_zYsY"
      ],
      [
        "Podcast with WaterCooler Neuroscience",
        "https://open.spotify.com/episode/6PYxjRFg2bI8pSDN82Dume?si=S7Z4BxXNS2aRCnbBMskTHg"
      ],
      [
        "Twitter",
        "https://twitter.com/neuralreckoning/status/1341011316975218695"
      ]
    ],
    "abstract": "The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been relatively little explored in models which are often highly homogeneous. We compared the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that heterogeneity substantially improved task performance. Learning with heterogeneity was more stable and robust, particularly for tasks with a rich temporal structure. In addition, the distribution of neuronal parameters in the trained networks is similar to those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.",
    "last_tweet_in_thread": "1341011376299511809",
    "video_embed": "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/aLNGcs_zYsY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>",
    "twitter_thread": "<blockquote class=\"twitter-tweet\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Have you ever wondered why neurons are like snowflakes? No two alike, even if they&#39;re the same type. In our latest preprint, we think we have (at least part of) the answer: it promotes robust learning.<br><br>Tweeprint follows (1/16)<a href=\"https://t.co/GbN9dXxt7Q\">https://t.co/GbN9dXxt7Q</a> <a href=\"https://t.co/1IQ1dvUYoZ\">pic.twitter.com/1IQ1dvUYoZ</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011316975218695?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">One of the striking things about the brain is how much diversity there is at so many levels, e.g. the distribution of membrane time constants. Check these out from <a href=\"https://twitter.com/AllenInstitute?ref_src=twsrc%5Etfw\">@AllenInstitute</a> and <a href=\"https://twitter.com/AuditoryNeuro?ref_src=twsrc%5Etfw\">@AuditoryNeuro</a> - wide range of values for single cell types, bit like a log normal dist. (2/16) <a href=\"https://t.co/O5gT0GSwPa\">pic.twitter.com/O5gT0GSwPa</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011323434504193?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">But, most models of networks of neurons that can carry out complex information processing tasks tend to use the same parameters for all neurons of the same type, with typically only connectivity being different for each neuron. (3/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011326945136642?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We guessed that networks of neurons with a wide range of time constants would be better at tasks where information is present at multiple time scales, such as auditory tasks like recognising speech. (4/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011329587527682?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Since we were interested in temporal dynamics and comparing to biology, we used spiking neural networks trained using <a href=\"https://twitter.com/hisspikeness?ref_src=twsrc%5Etfw\">@hisspikeness</a> <a href=\"https://twitter.com/virtualmind?ref_src=twsrc%5Etfw\">@virtualmind</a> surrogate gradient descent, adapted to learn time constants as well as weights. <a href=\"https://t.co/vq9BTqJ05F\">https://t.co/vq9BTqJ05F</a> (5/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011332175454212?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We found no improvement for N-MNIST which has little to no useful temporal information in the stimuli, some improvement for DVS gestures which does have temporal info but can be solved well without using it, and a huge improvement for recognising spoken digits. (6/16) <a href=\"https://t.co/tdwmP86uNd\">pic.twitter.com/tdwmP86uNd</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011337527386113?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The distributions of time constants learned are consistent for a given task, for each run, and regardless of whether you initialise time constants randomly or at a fixed value. And, they look like the distributions you find in the brain. (7/16) <a href=\"https://t.co/z32g3RtNuC\">pic.twitter.com/z32g3RtNuC</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011342921228289?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">And it&#39;s more robust. If you tune hyperparameters for sounds at a single speed, and then change the playback speed of stimuli, the heterogeneous networks can still learn the task but homogeneous ones start to fall down. (8/16) <a href=\"https://t.co/Otwkdon6Pb\">pic.twitter.com/Otwkdon6Pb</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011348130590720?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We also tested another training method, spiking FORCE of Nicola and <a href=\"https://twitter.com/ClopathLab?ref_src=twsrc%5Etfw\">@ClopathLab</a> and found the same robustness to hyperparameter mistuning. This fig shows the region where learning converges to a good solution in blue, axes are hyperparameters. (9/16) <a href=\"https://t.co/WfK1e66hD8\">pic.twitter.com/WfK1e66hD8</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011354275209216?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">So this looks to be pretty general: heterogeneity improves learning of temporal tasks and gives robustness against hyperparameter mistuning. And it does so at no metabolic cost. Same performance with homog networks requires 10x more neurons! So, surely the brain uses this (10/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011357282541576?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">This is also good for neuromorphic computing and ML: adding neuron level heterogeneity costs only O(n) time and memory, whereas adding more neurons and synapses costs O(n²). (11/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011359723630593?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">That&#39;s it for the results, we hope that this spurs people to look further into the importance of heterogeneity, e.g. spatial or cell type heterogeneity, and whether it can be useful in ML too. (12/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011362777083904?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">This work was done by two super talented PhD students Nicolas Perez and Vincent Cheung, neither of whom are on twitter unfortunately. (13/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011365729845259?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">And if you&#39;re interested in this area, there&#39;s a fascinating literature on heterogeneity, briefly reviewed in this paper, including similar work with a more neuromorphic angle from <a href=\"https://twitter.com/SanderBohte?ref_src=twsrc%5Etfw\">@SanderBohte</a>, Tim Masquelier. Nice review in <a href=\"https://twitter.com/GjorJulijana?ref_src=twsrc%5Etfw\">@GjorJulijana</a> <a href=\"https://t.co/VAsL9rDWmR\">https://t.co/VAsL9rDWmR</a> (14/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011369265672192?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">This work was only possible thanks to two incredible scientific developments. The first is new methods of training spiking neural networks from <a href=\"https://twitter.com/hisspikeness?ref_src=twsrc%5Etfw\">@hisspikeness</a> and others. We had a workshop on this over the summer, videos available at <a href=\"https://t.co/sNMvHiZsI3\">https://t.co/sNMvHiZsI3</a> (15/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011373036367872?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The second is the availability of incredible experimental datasets thanks to orgs like <a href=\"https://twitter.com/AllenInstitute?ref_src=twsrc%5Etfw\">@AllenInstitute</a>, <a href=\"https://t.co/jjSbYPf3T6\">https://t.co/jjSbYPf3T6</a> (<a href=\"https://twitter.com/neuronJoy?ref_src=twsrc%5Etfw\">@neuronJoy</a> <a href=\"https://twitter.com/rgerkin?ref_src=twsrc%5Etfw\">@rgerkin</a>) and individual labs like <a href=\"https://twitter.com/AuditoryNeuro?ref_src=twsrc%5Etfw\">@AuditoryNeuro</a>. Thank you so much for your generosity! (16/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1341011376299511809?ref_src=twsrc%5Etfw\">December 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
  },
  "jitter": {
    "year": 2021,
    "last_updated": "23-4-2021",
    "authors": "Su Y, Chung Y, Goodman DFM, Hancock KE, Delgutte B",
    "title": "Rate and Temporal Coding of Regular and Irregular Pulse Trains in Auditory Midbrain of Normal‑Hearing and Cochlear‑Implanted Rabbits",
    "journal": "Journal of the Association for Research in Otolaryngology",
    "doi": "10.1007/s10162-021-00792-5",
    "categories": [
      "Auditory",
      "Neuroscience"
    ],
    "urls": [
      [
        "Journal",
        "https://link.springer.com/article/10.1007/s10162-021-00792-5"
      ],
      [
        "Preprint (PDF)",
        "https://www.dropbox.com/s/jy45giagrffkm9j/jitter-preprint.pdf?dl=1"
      ]
    ],
    "abstract": "Although pitch is closely related to temporal periodicity, stimuli with a degree of temporal irregularity can evoke a pitch sensation in human listeners. However, the neural mechanisms underlying pitch perception for irregular sounds are poorly understood. Here, we recorded responses of single units in the inferior colliculus (IC) of normal hearing (NH) rabbits to acoustic pulse trains with different amounts of random jitter in the inter-pulse intervals and compared with responses to electric pulse trains delivered through a cochlear implant (CI) in a different group of rabbits. In both NH and CI animals, many IC neurons demonstrated tuning of firing rate to the average pulse rate (APR) that was robust against temporal jitter, although jitter tended to increase the firing rates for APRs ≥ 1280 Hz. Strength and limiting frequency of spike synchronization to stimulus pulses were also comparable between periodic and irregular pulse trains, although there was a slight increase in synchronization at high APRs with CI stimulation. There were clear differences between CI and NH animals in both the range of APRs over which firing rate tuning was observed and the prevalence of synchronized responses. These results suggest that the pitches of regular and irregular pulse trains are coded differently by IC neurons depending on the APR, the degree of irregularity, and the mode of stimulation. In particular, the temporal pitch produced by periodic pulse trains lacking spectral cues may be based on a rate code rather than a temporal code at higher APRs."
  },
  "nmc3": {
    "selected": true,
    "year": 2021,
    "authors": "Achakulvisut T, Ruangrong T, Mineault P, Vogels TP, Peters MAK, Poirazi P, Rozell C, Wyble B, Goodman DFM, Kording KP",
    "title": "Towards democratizing and automating online conferences: lessons from the Neuromatch conferences",
    "journal": "Trends in Cognitive Sciences",
    "doi": "10.1016/j.tics.2021.01.007",
    "categories": [
      "Neuroscience",
      "Machine learning",
      "Metascience"
    ],
    "urls": [
      [
        "Journal",
        "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(21)00009-7"
      ],
      [
        "Preprint (PDF)",
        "https://www.dropbox.com/s/snqgeuyt38vekfx/nmc3.pdf?dl=1"
      ],
      [
        "Neuromatch",
        "https://www.neuromatch.io/"
      ]
    ],
    "abstract": "Legacy conferences are costly, time-consuming, and exclude scientists lacking various resources\nor abilities. During the 2020 pandemic, we created an online conference platform, Neuromatch\nConferences, aimed at developing technological and cultural changes to make conferences more\ndemocratic, scalable, and accessible. We discuss the lessons we learned.",
    "peer_reviewed": false
  },
  "snufa_review": {
    "selected": true,
    "year": 2021,
    "authors": "Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F, Goodman DFM",
    "title": "Visualizing a joint future of neuroscience and neuromorphic engineering",
    "journal": "Neuron",
    "doi": "10.1016/j.neuron.2021.01.009",
    "categories": [
      "Neuroscience",
      "Spiking",
      "Machine learning",
      "Plasticity",
      "Learning",
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Journal",
        "https://doi.org/10.1016/j.neuron.2021.01.009"
      ],
      [
        "Preprint (PDF)",
        "https://www.dropbox.com/s/942rf97l80wyya5/snufa-meeting-report.pdf?dl=1"
      ],
      [
        "Workshop",
        "https://neural-reckoning.github.io/snn_workshop_2020/"
      ],
      [
        "Workshop talk recordings",
        "https://www.youtube.com/playlist?list=PL09WqqDbQWHFvM9DFYkM_GfnrVnIdLRhy"
      ],
      [
        "Twitter",
        "https://twitter.com/neuralreckoning/status/1362107086017036289"
      ]
    ],
    "abstract": "Recent research resolves the challenging problem of building biophysically plausible spiking neural models that\nare also capable of complex information processing. This advance creates new opportunities in neuroscience and\nneuromorphic engineering, which we discussed at an online focus meeting.",
    "last_tweet_in_thread": "1362107103998062594",
    "twitter_thread": "<blockquote class=\"twitter-tweet\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The spikes must flow!<br><br>I&#39;d love to announce a new paper with that title, but sadly the editors at Neuron changed it.<br><br>Still v happy this paper is out because there&#39;s a revolution taking place in spiking neural networks and I want everyone to know about it. 👇🧵 <a href=\"https://t.co/gXyEsrIJig\">pic.twitter.com/gXyEsrIJig</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107086017036289?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Two of the things that make the brain interesting are (a) it is intelligent, it lets us make sense of very complex, noisy sensory data, (b) neurons use this super weird method of communicating. Now, for the first time, we can train spiking networks that can do hard tasks.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107089020219392?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">This is a game changer! We can finally begin to answer questions about how the brain uses patterns of spikes to compute in real-world situations. This is the question that got me into neuroscience in the first place!</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107092140703744?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">So what changed? Methods from ML let us train neural networks at much harder tasks than before, but this was limited to artificial NNs, not spiking. Over the last couple of years, a number of tricks have been found to make it work for general case spiking neurons.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107094464348160?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The code is relatively easy to write, but it&#39;s still quite slow at the moment and can only be used for a few hundred neurons. But, this is changing rapidly and there are going to be exciting times ahead over the next few years.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107096913874947?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">So, go take a look at our paper. This link will work until April 8th as long as you turn off your ad blocker:<a href=\"https://t.co/Btrhf9MGp4\">https://t.co/Btrhf9MGp4</a><br><br>After that, I&#39;ll keep this page up to date:<a href=\"https://t.co/mhBRunEpMb\">https://t.co/mhBRunEpMb</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107098889412609?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">You can also take a look at recordings of the talks this review paper is based on here:<a href=\"https://t.co/sNMvHiZsI3\">https://t.co/sNMvHiZsI3</a><br><br>It was based around a workshop we ran last summer, and we&#39;re planning to run that again this summer, this time with a challenge, so keep an eye out for that.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107101762506756?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Thanks for your attention, and to co-authors <a href=\"https://twitter.com/hisspikeness?ref_src=twsrc%5Etfw\">@hisspikeness</a> <a href=\"https://twitter.com/SanderBohte?ref_src=twsrc%5Etfw\">@SanderBohte</a> <a href=\"https://twitter.com/ClopathLab?ref_src=twsrc%5Etfw\">@ClopathLab</a> <a href=\"https://twitter.com/astronomind?ref_src=twsrc%5Etfw\">@astronomind</a> <a href=\"https://twitter.com/NeuroNaud?ref_src=twsrc%5Etfw\">@NeuroNaud</a> <a href=\"https://twitter.com/virtualmind?ref_src=twsrc%5Etfw\">@virtualmind</a> <a href=\"https://twitter.com/franz_scherr?ref_src=twsrc%5Etfw\">@franz_scherr</a>  and others not on twitter. (End)</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1362107103998062594?ref_src=twsrc%5Etfw\">February 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
  },
  "sparse_spiking_gradient_descent": {
    "year": 2021,
    "last_updated": "12-1-2022",
    "authors": "Perez-Nieves N, Goodman DFM",
    "title": "Sparse spiking gradient descent",
    "journal": "Advances in Neural Information Processing Systems",
    "additional": "34",
    "categories": [
      "Machine learning",
      "Spiking",
      "Neural simulation",
      "Learning"
    ],
    "urls": [
      [
        "Journal",
        "https://proceedings.neurips.cc/paper/2021/hash/61f2585b0ebcf1f532c4d1ec9a7d51aa-Abstract.html"
      ],
      [
        "PDF",
        "https://proceedings.neurips.cc/paper/2021/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/2105.08810"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2105.08810"
      ]
    ],
    "abstract": "There is an increasing interest in emulating Spiking Neural Networks (SNNs) on neuromorphic\ncomputing devices due to their low energy consumption. Recent advances have allowed training\nSNNs to a point where they start to compete with traditional Artificial Neural Networks (ANNs)\nin terms of accuracy, while at the same time being energy efficient when run on neuromorphic\nhardware. However, the process of training SNNs is still based on dense tensor operations\noriginally developed for ANNs which do not leverage the spatiotemporally sparse nature of SNNs.\nWe present here the first sparse SNN backpropagation algorithm which achieves the same or better\naccuracy as current state of the art methods while being significantly faster and more memory\nefficient. We show the effectiveness of our method on real datasets of varying complexity\n(Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a speedup in the\nbackward pass of up to 150x, and 85% more memory efficient, without losing accuracy."
  },
  "weerts_thesis": {
    "year": 2021,
    "authors": "Weerts L",
    "title": "Features of hearing: applications of machine learning to uncover the building blocks of hearing",
    "publisher": "Imperial College London",
    "phd_thesis": true,
    "categories": [
      "Machine learning",
      "Auditory",
      "Neuroscience",
      "Software"
    ],
    "urls": [
      [
        "Thesis",
        "https://spiral.imperial.ac.uk/handle/10044/1/100667"
      ]
    ],
    "abstract": "Recent advances in machine learning have instigated a renewed interest in using machine learning approaches to better understand human sensory processing. This line of research is particularly interesting for speech research since speech comprehension is uniquely human, which complicates obtaining detailed neural recordings. In this thesis, I explore how machine learning can be used to uncover new knowledge about the auditory system, with a focus on discovering robust auditory features. The resulting increased understanding of the noise robustness of human hearing may help to better assist those with hearing loss and improve Automatic Speech Recognition (ASR) systems. First, I show how computational neuroscience and machine learning can be combined to generate hypotheses about auditory features. I introduce a neural feature detection model with a modest number of parameters that is compatible with auditory physiology. By testing feature detector variants in a speech classification task, I confirm the importance of both well-studied and lesser-known auditory features. Second, I investigate whether ASR software is a good candidate model of the human auditory system. By comparing several state-of-the-art ASR systems to the results from humans on a range of psychometric experiments, I show that these ASR systems diverge markedly from humans in at least some psychometric tests. This implies that none of these systems act as a strong proxy for human speech recognition, although some may be useful when asking more narrowly defined questions. For neuroscientists, this thesis exemplifies how machine learning can be used to generate new hypotheses about human hearing, while also highlighting the caveats of investigating systems that may work fundamentally differently from the human brain. For machine learning engineers, I point to tangible directions for improving ASR systems. To motivate the continued cross-fertilization between these fields, a toolbox that allows researchers to assess new ASR systems has been released.",
    "software": [
      "humanlike_hearing"
    ]
  },
  "zheng_thesis": {
    "year": 2021,
    "authors": "Zheng JX",
    "title": "Advances in network visualisation with an application to serious games",
    "publisher": "Imperial College London",
    "phd_thesis": true,
    "categories": [
      "Machine learning",
      "Visualisation",
      "Software",
      "Ecology"
    ],
    "urls": [
      [
        "Thesis",
        "https://spiral.imperial.ac.uk/handle/10044/1/88583"
      ]
    ],
    "abstract": "This thesis concerns the visualisation of networks, through an in-depth study into\nthe node-link diagram representation. Three subtopics are explored within this\nspace. The first is the problem of node layout, where the optimisation of a popular\nenergy function, known as stress, is improved through an algorithm known\nas stochastic gradient descent. The second is the method of edge bundling, where\nthe idea of hierarchical edge bundling is explored in the absence of a known ground\ntruth hierarchy. Its similarity to a topologically lossless bundling method known\nas power-confluent drawing is then leveraged, in order to improve technical problems\nwith the underlying algorithms. The final topic is an engineering application\nin the form of a serious game called EcoBuilder, which utilises the node-link diagram\nto visualise the dynamical behaviour of food webs. Its purpose is to crowdsource\nresearch through a citizen science approach, with outcomes in both visualisation\nand mathematical ecology.",
    "software": [
      "ecobuilder",
      "sgd2",
      "pconfluent"
    ]
  },
  "assessing_hrtf_ambisonics": {
    "year": 2022,
    "last_updated": "26-1-2022",
    "authors": "Engel I, Goodman DFM, Picinali L",
    "title": "Assessing HRTF preprocessing methods for Ambisonics rendering through perceptual models",
    "journal": "Acta Acustica",
    "categories": [
      "Virtual reality",
      "Auditory"
    ],
    "urls": [
      [
        "Journal",
        "https://acta-acustica.edpsciences.org/articles/aacus/abs/2022/01/aacus210029/aacus210029.html"
      ],
      [
        "Journal (PDF)",
        "https://acta-acustica.edpsciences.org/articles/aacus/pdf/2022/01/aacus210029.pdf"
      ]
    ],
    "doi": "10.1051/aacus/2021055",
    "abstract": "Binaural rendering of Ambisonics signals is a common way to reproduce spatial audio content.\nProcessing Ambisonics signals at low spatial orders is desirable in order to reduce complexity,\nalthough it may degrade the perceived quality, in part due to the mismatch that occurs when a\nlow-order Ambisonics signal is paired with a spatially dense head-related transfer function (HRTF).\nIn order to alleviate this issue, the HRTF may be preprocessed so its spatial order is reduced.\nSeveral preprocessing methods have been proposed, but they have not been thoroughly compared yet.\nIn this study, nine HRTF preprocessing methods were used to render anechoic binaural signals from\nAmbisonics representations of orders 1 to 44, and these were compared through perceptual hearing\nmodels in terms of localisation performance, externalisation and speech reception. This assessment\nwas supported by numerical analyses of HRTF interpolation errors, interaural differences,\nperceptually-relevant spectral differences, and loudness stability. Models predicted that the\nbinaural renderings’ accuracy increased with spatial order, as expected. A notable effect of the\npreprocessing method was observed: whereas all methods performed similarly at the highest spatial\norders, some were considerably better at lower orders. A newly proposed method, BiMagLS, displayed\nthe best performance overall and is recommended for the rendering of bilateral Ambisonics signals.\nThe results, which were in line with previous literature, indirectly validate the perceptual models’\nability to predict listeners’ responses in a consistent and explicable manner."
  },
  "cosyne_tutorial": {
    "year": 2022,
    "last_updated": "2-9-2022",
    "authors": "Goodman D, Fiers T, Gao R, Ghosh M, Perez N",
    "journal": "Zenodo",
    "title": "Spiking Neural Network Models in Neuroscience - Cosyne Tutorial 2022",
    "categories": [
      "Spiking",
      "Machine learning",
      "Modelling"
    ],
    "urls": [
      [
        "Tutorial website",
        "https://neural-reckoning.github.io/cosyne-tutorial-2022/"
      ],
      [
        "Tutorial videos (YouTube)",
        "https://www.youtube.com/playlist?list=PL09WqqDbQWHGJd7Il3yVxiBts5nRSxvJ4"
      ]
    ],
    "doi": "10.5281/zenodo.7044500",
    "abstract": "Tutorial given at Cosyne 2022, on spiking neural networks from a classical and machine learning perspective.",
    "peer_reviewed": false
  },
  "humanlikehearing": {
    "year": "Preprints",
    "last_updated": "21-04-2022",
    "authors": "Weerts L, Rosen S, Clopath C, Goodman DFM",
    "title": "The Psychometrics of Automatic Speech Recognition",
    "categories": [
      "Neuroscience",
      "Auditory",
      "Machine learning",
      "Modelling",
      "Software",
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/2021.04.19.440438v3"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/2021.04.19.440438v3.full.pdf"
      ],
      [
        "Code",
        "https://github.com/neural-reckoning/HumanlikeHearing"
      ],
      [
        "Twitter",
        "https://twitter.com/neuralreckoning/status/1395704493753479168"
      ],
      [
        "Talk by Dan Goodman (video)",
        "https://www.youtube.com/watch?v=TnTphoFWsrE"
      ]
    ],
    "abstract": "Deep neural networks have had considerable success in neuroscience as models of the visual system, and recent work has suggested this may also extend to the auditory system. We tested the behaviour of a range of state of the art deep learning-based automatic speech recognition systems on a wide collection of manipulated sounds used in standard human psychometric experiments. While some systems showed qualitative agreement with humans in certain tests, in others all tested systems diverged markedly from humans. In particular, all systems used spectral invariance, temporal fine structure and speech periodicity differently from humans. We conclude that despite some promising results, none of the tested automatic speech recognition systems can yet act as a strong proxy for human speech recognition. However, we note that the more recent systems with better performance also tend to better match human results, suggesting that continued cross-fertilisation of ideas between human and automatic speech recognition may be fruitful. Our open source toolbox allows researchers to assess future automatic speech recognition systems or add additional psychoacoustic measures.",
    "last_tweet_in_thread": "1395704552519839746",
    "video_embed": "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/TnTphoFWsrE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>",
    "software": [
      "humanlike_hearing"
    ],
    "twitter_thread": "<blockquote class=\"twitter-tweet\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Are deep nets good brain models? Psychometric testing of automatic speech recognition systems shows they&#39;re not like humans, yet. Newer ones getting closer, but still ignore important cues.<br><br>Preprint with Lotte Weerts, Stuart Rosen and <a href=\"https://twitter.com/ClopathLab?ref_src=twsrc%5Etfw\">@ClopathLab</a>. <a href=\"https://t.co/JAdHY9bkzD\">https://t.co/JAdHY9bkzD</a><br><br>🧵👇</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704493753479168?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We&#39;re excited about the huge performance leaps in vision and hearing DNNs, and how they&#39;re being used as models of the visual and auditory system, e.g. <a href=\"https://twitter.com/JoshHMcDermott?ref_src=twsrc%5Etfw\">@JoshHMcDermott</a> (<a href=\"https://t.co/ZqTxQY9q47\">https://t.co/ZqTxQY9q47</a>) and <a href=\"https://twitter.com/HearingTechLab?ref_src=twsrc%5Etfw\">@HearingTechLab</a> (<a href=\"https://t.co/ueeSHQfE1w\">https://t.co/ueeSHQfE1w</a>). But do these models work like people?</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704495909347328?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">TL;DR: Automatic speech recognisers are less robust than people at distorted speech, and seem to use different cues (particularly temporal fine structure and periodicity). The most recent model we tested - <a href=\"https://twitter.com/facebookai?ref_src=twsrc%5Etfw\">@facebookai</a> Wav2Vec - was the closest to humans, and also the best overall</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704498757242887?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">As well as being the best, most robust model, Wav2Vec was also the only end-to-end trained model not using a hand-designed speech recognition front-end (MFCC features). This raises questions about the validity of these front-ends and is a hopeful sign for future developments!</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704500812492800?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">So although current models are not good enough to be used as proxies for the auditory system, we expect this to change as these models improve. We provide our benchmarks as an open source library HumanlikeHearing to make it easy to test future systems:<a href=\"https://t.co/isnCbhQeZG\">https://t.co/isnCbhQeZG</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704502901264389?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Now, let&#39;s get on to some of the gory details. There&#39;s a lot more in the paper if you&#39;re a sucker for punishment. 😉</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704505120006146?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We tested three recent automatic speech recognition (ASR) systems on a range of psychometric tests designed for humans, to compare overall performance, patterns of errors, and work out which auditory cues they were using (think texture vs shape for vision).</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704507162583041?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The ASR systems we tested were:<br>👂 Zurow&#39;s Kaldi nnet3, a DNN-HMM hybrid<br>👂 <a href=\"https://twitter.com/mozilla?ref_src=twsrc%5Etfw\">@Mozilla</a> DeepSpeech, based on LSTMs<br>👂 <a href=\"https://twitter.com/facebookai?ref_src=twsrc%5Etfw\">@facebookai</a> Wav2Vec, a CNN-Transformer model<br>The first two use a standard speech recognition front-end (MFCCs) while Wav2Vec is trained end-to-end (important 👇).</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704509310046208?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">First up: how well do they work with a reduced frequency range? Answer: not well. Humans reach ceiling performance with just 12 semitones around 1.5kHz, while even the best ASR needed around 40. Note that CNN-Transformer did the best here: you&#39;ll see that again. <a href=\"https://t.co/HHNWPKdduF\">pic.twitter.com/HHNWPKdduF</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704513571524616?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Next up: peak and centre clipping. Peak clipping is what happens when your microphone saturates, and centre clipping with some noise suppression systems. ASRs all perform badly with peak clipping, but CNN-Transformer and DNN-HMM match really well for centre clipping. <a href=\"https://t.co/NgBP4M3Wmg\">pic.twitter.com/NgBP4M3Wmg</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704517971304451?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We looked at how ASR systems use spectral and temporal modulations, following <a href=\"https://twitter.com/TheunissenLab?ref_src=twsrc%5Etfw\">@TheunissenLab</a> method for removing certain modulations. They&#39;re overall less robust (we had to use higher SNR to get comparable results), but seem to be using these modulations in a similar way. <a href=\"https://t.co/5LoYX7cfkY\">pic.twitter.com/5LoYX7cfkY</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704523080052736?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Sounds have slow (envelope) and fast (temporal fine structure) components. TFS has been suggested to be important for hearing in noisy environments. We expected the end-to-end CNN-Transformer might use this better than the systems using MFCCs which mostly discard TFS, but no. <a href=\"https://t.co/8sgmP1oW4A\">pic.twitter.com/8sgmP1oW4A</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704527983108102?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Similarly, none of the ASR systems seem to use periodicity information in the same way as humans. They aren&#39;t as robust to distortion and don&#39;t show the same patterns of errors for different distortions. <a href=\"https://t.co/KZWkwoEhwG\">pic.twitter.com/KZWkwoEhwG</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704533200875532?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">One of the big challenges for both humans and ASR systems is handling competing talkers. Although they perform less well, requiring higher SNRs, the DNN-HMM and CNN-Transformer show a similar pattern to humans, &quot;glimpsing&quot; signals in dips in the noise. <a href=\"https://t.co/dgnwafnegP\">pic.twitter.com/dgnwafnegP</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704537844011014?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Digging deeper into this, humans can benefit from both periodicity and fluctuations in masking noise. Of the ASR systems, only CNN-Transformer shows a benefit from both and a somewhat similar trend. <a href=\"https://t.co/wRwnqYB0O3\">pic.twitter.com/wRwnqYB0O3</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704542172483587?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Finally, you might ask if these comparisons are fair because these models are all trained with clean speech. We fine tuned CNN-Transformer with bandpass filtered speech and it improved performance on that test, but made noise robustness worse. <a href=\"https://t.co/GBKGVGYmji\">pic.twitter.com/GBKGVGYmji</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704546471653377?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We could probably improve by fine tuning across all our tests, but this isn&#39;t really the point. Most of the distortions we tested are ones that human listeners haven&#39;t previously encountered either.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704548874985474?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">In summary, the ASR systems are quite different to humans, but end-to-end training with CNN-Transformer lets it get a lot closer. If humans are a guide, future models may benefit from making more use of TFS and periodicity information.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704550686855174?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-width=\"400\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">And if you&#39;ve made it all the way to here, congratulations! We&#39;d love to get feedback on the paper, and if you have a go at using our code, let us know (and file bug reports if you find any!).<br><br>Thank you for reading. 😊</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1395704552519839746?ref_src=twsrc%5Etfw\">May 21, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
  },
  "perez_thesis": {
    "year": 2023,
    "authors": "Perez N",
    "title": "Robust and efficient training on deep spiking neural networks",
    "publisher": "Imperial College London",
    "phd_thesis": true,
    "categories": [
      "Machine learning",
      "Neuroscience",
      "Software",
      "Spiking",
      "Learning",
      "Modelling",
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Thesis",
        "https://spiral.imperial.ac.uk/handle/10044/1/112873"
      ]
    ],
    "abstract": "This thesis focuses on the study of training deep spiking neural networks (SNNs). In recent years there has been an increasing interest in using spiking neurons for deep learning with the aim of leveraging their unique properties and characteristics. These include their potential for very energy efficient training and inference due to their highly sparse activity and their suitability to model biological neurons. We first introduce the fundamental models that are used in the SNN training literature, including neuron models at different levels of abstraction, synapses and networks of neurons and neurons under noise. We also review the main SNN training methods that have been developed to this date. Then, we study the role of neural heterogeneity and study the performance and robustness of SNNs under different heterogeneity schemes on two different supervised learning methods. Next, we show how the sparse activity present in the forward pass on SNNs can also be achieved in the backward pass, leading to highly efficient implementations that can speed up the backward pass up to 150x and save 85% of the memory. Finally, we aim to solve the weight initialisation problem for SNNs to achieve a predictable network activity as well as prevent the gradient from vanishing or exploding. In this process, we identify and solve the firing rate collapse issue caused by the discretisation of SNNs for simulation. In addition, we obtain theoretical and empirical results for a general SNN initialisation strategy making use of variance propagation and diffusion/shot-noise/threshold integration methods, as well as the solution to the firing rate collapse problem we previously found. Besides the ideas and experiments discussed in this thesis, code for the methods described here can be found in <a href=\"https://github.com/npvoid\">https://github.com/npvoid</a>."
  },
  "snn_init": {
    "year": "Preprints",
    "last_updated": "17-05-2023",
    "authors": "Perez N, Goodman DFM",
    "title": "Spiking Network Initialisation and Firing Rate Collapse",
    "categories": [
      "Neural simulation",
      "Machine learning"
    ],
    "urls": [
      [
        "Preprint",
        "https://arxiv.org/abs/2305.08879"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2305.08879"
      ]
    ],
    "abstract": "In recent years, newly developed methods to train spiking neural networks (SNNs) have rendered them as a\nplausible alternative to Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time\nbeing much more energy efficient at inference and potentially at training time. However, it is still unclear\nwhat constitutes a good initialisation for an SNN. We often use initialisation schemes developed for ANN\ntraining which are often inadequate and require manual tuning. In this paper, we attempt to tackle this\nissue by using techniques from the ANN initialisation literature as well as computational neuroscience\nresults. We show that the problem of weight initialisation for ANNs is a more nuanced problem than it is\nfor ANNs due to the spike-and-reset non-linearity of SNNs and the firing rate collapse problem. We firstly\nidentify and propose several solutions to the firing rate collapse problem under different sets of\nassumptions which successfully solve the issue by leveraging classical random walk and Wiener processes\nresults. Secondly, we devise a general strategy for SNN initialisation which combines variance propagation\ntechniques from ANNs and different methods to obtain the expected firing rate and membrane potential\ndistribution based on diffusion and shot-noise approximations. Altogether, we obtain theoretical results\nto solve the SNN initialisation which consider the membrane potential distribution in the presence of a\nthreshold. Yet, to what extent can these methods be successfully applied to SNNs on real datasets remains\nan open question. "
  },
  "adapting_to_time": {
    "year": 2024,
    "last_updated": "13-12-2024",
    "authors": "Habashy KG, Evans BD, Goodman DFM, Bowers JS",
    "title": "Adapting to time: why nature may have evolved a diverse set of neurons",
    "journal": "PLoS Computational Biology",
    "doi": "10.1371/journal.pcbi.1012673",
    "additional": "20(12): e1012673",
    "categories": [
      "Neuroscience",
      "Learning",
      "Modelling",
      "Machine learning",
      "Spiking"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012673"
      ],
      [
        "Journal (PDF)",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1012673&type=printable"
      ],
      [
        "Code (GitHub)",
        "https://github.com/biocomplab/Neuro-morphology"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/2404.14325"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2404.14325"
      ]
    ],
    "abstract": "Brains have evolved diverse neurons with varying morphologies and dynamics that impact temporal information processing. In contrast, most neural network models use homogeneous units that vary only in spatial parameters (weights and biases). To explore the importance of temporal parameters, we trained spiking neural networks on tasks with varying temporal complexity, holding different parameter subsets constant. We found that adapting conduction delays is crucial for solving all test conditions under tight resource constraints. Remarkably, these tasks can be solved using only temporal parameters (delays and time constants) with constant weights. In more complex spatio-temporal tasks, an adaptable bursting parameter was essential. Overall, allowing adaptation of both temporal and spatial parameters enhances network robustness to noise, a vital feature for biological brains and neuromorphic computing systems. Our findings suggest that rich and adaptable dynamics may be the key for solving temporally structured tasks efficiently in evolving organisms, which would help explain the diverse physiological properties of biological neurons."
  },
  "multimodal": {
    "selected": true,
    "year": 2024,
    "last_updated": "05-07-2024",
    "authors": "Ghosh M, Béna G, Bormuth V, Goodman DFM",
    "title": "Nonlinear fusion is optimal for a wide class of multisensory tasks",
    "journal": "PLoS Computational Biology",
    "additional": "20(7): e1012246",
    "doi": "10.1371/journal.pcbi.1012246",
    "categories": [
      "Sensory",
      "Neuroscience",
      "Modelling",
      "Spiking",
      "Multimodal"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://doi.org/10.1371/journal.pcbi.1012246"
      ],
      [
        "Journal (PDF)",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1012246&type=printable"
      ],
      [
        "Code (Zenodo)",
        "https://doi.org/10.5281/zenodo.12191565"
      ],
      [
        "Code (GitHub)",
        "http://github.com/ghoshm/Nonlinear_fusion"
      ],
      [
        "Preprint (biorxiv)",
        "https://www.biorxiv.org/content/10.1101/2023.07.24.550311v2"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/2023.07.24.550311v2.full.pdf"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/110785811144218374"
      ],
      [
        "Twitter",
        "https://twitter.com/neuralreckoning/status/1684525463530528768"
      ]
    ],
    "abstract": "Animals continuously detect information via multiple sensory channels, like vision and hearing, and integrate these signals to realise faster and more accurate decisions; a fundamental neural computation known as multisensory integration. A widespread view of this process is that multimodal neurons linearly fuse information across sensory channels. However, does linear fusion generalise beyond the classical tasks used to explore multisensory integration? Here, we develop novel multisensory tasks, which focus on the underlying statistical relationships between channels, and deploy models at three levels of abstraction: from probabilistic ideal observers to artificial and spiking neural networks. Using these models, we demonstrate that when the information provided by different channels is not independent, linear fusion performs sub-optimally and even fails in extreme cases. This leads us to propose a simple nonlinear algorithm for multisensory integration which is compatible with our current knowledge of multimodal circuits, excels in naturalistic settings and is optimal for a wide class of multisensory tasks. Thus, our work emphasises the role of nonlinear fusion in multisensory integration, and provides testable hypotheses for the field to explore at multiple levels: from single neurons to behaviour.",
    "twitter_thread": "<blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">New preprint! A simple way to extend the classical evidence weighting model of multimodal integration to solve a much wider range of naturalistic tasks. Spoiler: it&#39;s nonlinearity. Works for SNNs/ANNs. 🧵 with <a href=\"https://twitter.com/MarcusGhosh?ref_src=twsrc%5Etfw\">@MarcusGhosh</a> <a href=\"https://twitter.com/GabrielBna1?ref_src=twsrc%5Etfw\">@GabrielBna1</a> <a href=\"https://twitter.com/BormuthVolker?ref_src=twsrc%5Etfw\">@BormuthVolker</a> <a href=\"https://t.co/4to71pOfsd\">https://t.co/4to71pOfsd</a> <a href=\"https://t.co/E3ty5nlyp1\">pic.twitter.com/E3ty5nlyp1</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525463530528768?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Think about the infamous &#39;cocktail party&#39;: you use synchrony between lip movements and sounds to help you hear in a noisy environment. But the classical model throws away that temporal structure, instead just linearly weighting visual and auditory evidence.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525465912975361?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We call this algorithm accumulate-then-fuse because first you accumulate evidence over time within a modality, followed by linearly fusing across modalities. We propose instead to (nonlinearly) fuse-then-accumulate. This works much better with pretty much any nonlinearity. <a href=\"https://t.co/caUnKVqNW6\">pic.twitter.com/caUnKVqNW6</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525467649409024?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">This work started when we were training spiking neural networks with surrogate gradient descent (thanks <a href=\"https://twitter.com/hisspikeness?ref_src=twsrc%5Etfw\">@hisspikeness</a>) to solve the classical multimodal task where multimodal signals are independent. To our surprise, we didn&#39;t need a multimodal area to solve this task! <a href=\"https://t.co/1RxQ5lxG1d\">pic.twitter.com/1RxQ5lxG1d</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525469666795520?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">In our comodulation tasks the evidence within a modality is forced to be balanced, and only the joint temporal structure carries information. Sure enough, we found you need a multimodal area to do this task (and in unpublished pilot data, the humans in our lab can do this task). <a href=\"https://t.co/1eTeK1CnRc\">pic.twitter.com/1eTeK1CnRc</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525471764041732?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">But this task is kind of unrealistic so we designed a &quot;detection task&quot; where the signal is only on at unknown times, the rest of the time you get noise. You can do this with or without a multimodal area, but there are big differences in performance when the signal is sparse. <a href=\"https://t.co/CVkzKKpjOM\">pic.twitter.com/CVkzKKpjOM</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525473760432128?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">This seems likely to be important in natural settings because fast and accurate reactions to sparse information could make all the difference in a predator-prey interaction. 🐈🐁 And the more complex the task, the bigger the performance difference.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525475866071040?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The optimal nonlinearity is softplus(x)=log(1+be^cx) but training artificial neural networks with different nonlinearities like ReLU or sigmoid is just as good in practice. The solution extends to continuous observations, eg. for Gaussian noise you need softplus and quadratic.</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525477514428417?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Can we relate this to experimental data? One measure used is additivity: how much neurons respond to multimodal signals than you&#39;d guess from unimodal responses. We found high additivity was more important in tasks where FtA did better than AtF, largely due to time constants. <a href=\"https://t.co/2jtBGtAgpZ\">pic.twitter.com/2jtBGtAgpZ</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525479242387458?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Plus, we can look at behaviour. In our sparse detection task we can predict which trials subjects are likely to make mistakes on if they use AtF rather than FtA (by plotting trials based on weight of evidence assuming AtF=x or FtA=y). <a href=\"https://t.co/6JNUEQvNms\">pic.twitter.com/6JNUEQvNms</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525481205407747?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We haven&#39;t done the experiments to prove this is what we do (yet), but:<br>⭐ It&#39;s consistent with previous experiments (as it is a generalisation of AtF)<br>⭐ It&#39;s the solution found when training spiking or artificial NNs<br>⭐ It gives better performance with few extra parameters</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525483248029696?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  <blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">For more details check out the beautiful HTML version of the preprint on <a href=\"https://twitter.com/curvenote?ref_src=twsrc%5Etfw\">@curvenote</a> (many thanks for the support!):<a href=\"https://t.co/aSclRRJQzn\">https://t.co/aSclRRJQzn</a><br><br>or the good old PDF at <a href=\"https://twitter.com/biorxivpreprint?ref_src=twsrc%5Etfw\">@biorxivpreprint</a>:<a href=\"https://t.co/4to71pOfsd\">https://t.co/4to71pOfsd</a><br><br>Let us know what you think!</p>&mdash; Dan Goodman (@neuralreckoning) <a href=\"https://twitter.com/neuralreckoning/status/1684525485882068992?ref_src=twsrc%5Etfw\">July 27, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n"
  },
  "comob_soundloc": {
    "year": 2025,
    "last_updated": "26-06-2025",
    "authors": "Ghosh M, Habashy KG, De Santis F, Fiers T, Erçelik DF, Mészáros B, Friedenberger Z, Béna G, Hong M, Abubacar U, Byrne RT, Riquelme JL, Liu YH, Aizenbud I, Bicknell BA, Bormuth V, Antonietti A, Goodman DFM",
    "title": "Spiking neural network models of interaural time difference extraction via a massively collaborative process",
    "journal": "eNeuro",
    "additional": "12 (7) ENEURO.0383-24.2025",
    "categories": [
      "Machine learning",
      "Auditory",
      "Sound localisation",
      "Modelling",
      "Neuroscience",
      "Learning",
      "Spiking",
      "Metascience"
    ],
    "doi": "https://doi.org/10.1523/ENEURO.0383-24.2025",
    "urls": [
      [
        "Journal (HTML)",
        "https://doi.org/10.1523/ENEURO.0383-24.2025"
      ],
      [
        "Journal (PDF)",
        "https://www.eneuro.org/content/eneuro/12/7/ENEURO.0383-24.2025.full.pdf"
      ],
      [
        "Working preprint (HTML)",
        "https://comob-project.github.io/snn-sound-localization/paper"
      ],
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/2024.07.19.604252v1"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/2024.07.19.604252v1.full.pdf"
      ],
      [
        "GitHub repository",
        "https://github.com/comob-project/snn-sound-localization"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/neural-reckoning.org/post/3lxzfv5yz4k2x"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/115146644216055627"
      ]
    ],
    "abstract": "Neuroscientists are increasingly initiating large-scale collaborations which bring together tens to hundreds of researchers. At this scale, such projects can tackle large-scale challenges and engage a wide range of participants. Inspired by projects in pure mathematics, we set out to test the feasibility of widening access to such projects even further, by running a massively collaborative project in computational neuroscience. The key difference, with prior neuroscientific efforts, being that our entire project (code, results, writing) was public from the outset, and that anyone could participate. To achieve this, we launched a public Git repository, with code for training spiking neural networks to solve a sound localisation task via surrogate gradient descent. We then invited anyone, anywhere to use this code as a springboard for exploring questions of interest to them, and encouraged participants to share their work both asynchronously through Git and synchronously at monthly online workshops. Our hope was that the resulting range of participants would allow us to make discoveries that a single team would have been unlikely to find. At a scientific level, our work investigated how a range of biologically-relevant parameters, from time delays to membrane time constants and levels of inhibition, could impact sound localisation in networks of spiking units. At a more macro-level, our project brought together 31 researchers from multiple countries, provided hands-on research experience to early career participants, and opportunities for supervision and teaching to later career participants. While our scientific results were not groundbreaking, our project demonstrates the potential for massively collaborative projects to transform neuroscience.",
    "bluesky_thread": [
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfv5yz4k2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfv7qoek2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvarg7s2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvarg7t2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvbshtl2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvcodgt2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvczrnl2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvdyf5d2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvdyg4l2x",
      "https://bsky.app/profile/neural-reckoning.org/post/3lxzfvdyh3t2x"
    ]
  },
  "fund_ecrs_not_moonshots": {
    "year": 2025,
    "last_updated": "26-05-2025",
    "authors": "Goodman D",
    "title": "Neuroscience needs to empower early-career researchers, not fund moon shots",
    "journal": "The Transmitter",
    "categories": [
      "Metascience"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://www.thetransmitter.org/funding/neuroscience-needs-to-empower-early-career-researchers-not-fund-moon-shots/"
      ]
    ],
    "abstract": "Large-scale projects run the risk of stifling scientific independence. Instead, let's explore alternative mechanisms of collaboration.",
    "peer_reviewed": false
  },
  "international_science_infrastructure": {
    "year": 2025,
    "last_updated": "20-02-2025",
    "authors": "Goodman D",
    "title": "Science must step away from nationally managed infrastructure",
    "journal": "The Transmitter",
    "categories": [
      "Metascience"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://www.thetransmitter.org/policy/science-must-step-away-from-nationally-managed-infrastructure/"
      ]
    ],
    "abstract": "Scientific data and independence are at risk. We need to work with community-driven services and university libraries to create new multi-country organizations that are resilient to political interference.",
    "peer_reviewed": false
  },
  "learning_sound_loc_limited_sup": {
    "year": 2025,
    "last_updated": "10-10-2025",
    "authors": "Chu Y, Luk W, Goodman DFM",
    "title": "Learning spatial hearing via innate mechanisms",
    "journal": "PLoS Computational Biology",
    "additional": "21(10): e1013543",
    "categories": [
      "Machine learning",
      "Auditory",
      "Sound localisation",
      "Modelling",
      "Neuroscience",
      "Learning"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1013543"
      ],
      [
        "Journal (PDF)",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1013543&type=printable"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/2001.10605"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2001.10605"
      ],
      [
        "Code (GitHub)",
        "https://github.com/YangTrue/Learning-spatial-hearing-via-innate-mechanisms"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/neural-reckoning.org/post/3lnl6iqypp22g"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/114394002639348151"
      ]
    ],
    "doi": "10.1371/journal.pcbi.1013543",
    "abstract": "The acoustic cues used by humans and other animals to localise sounds are subtle, and change throughout our lifetime. This means that we need to constantly relearn or recalibrate our sound localisation circuit. This is often thought of as a “supervised” learning process where a “teacher” (for example, a parent, or your visual system) tells you whether or not you guessed the location correctly, and you use this information to update your localiser. However, there is not always an obvious teacher (for example in babies or blind people). Using computational models, we showed that approximate feedback from a simple innate circuit, such as that can distinguish left from right (e.g. the auditory orienting response), is sufficient to learn an accurate full-range sound localiser. Moreover, using this mechanism in addition to supervised learning can more robustly maintain the adaptive neural representation. We find several possible neural mechanisms that could underlie this type of learning, and hypothesise that multiple mechanisms may be present and provide examples in which these mechanisms can interact with each other. We conclude that when studying spatial hearing, we should not assume that the only source of learning is from the visual system or other supervisory signals. Further study of the proposed mechanisms could allow us to design better rehabilitation programmes to accelerate relearning/recalibration of spatial hearing.",
    "bluesky_thread": [
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6iqypp22g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6is233c2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6is242k2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6is242l2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6is24zt2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6isrbbd2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6itoqmt2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6iuqnll2g",
      "https://bsky.app/profile/neural-reckoning.org/post/3lnl6iuqpk32g"
    ]
  },
  "multisensory-channels-and-time": {
    "year": 2025,
    "last_updated": "10-06-2025",
    "authors": "Anil S, Goodman DFM, Ghosh M",
    "title": "Fusing multisensory signals across channels and time",
    "journal": "PLoS Computational Biology",
    "doi": "10.1371/journal.pcbi.1013125",
    "additional": " 21(6): e1013125",
    "categories": [
      "Sensory",
      "Neuroscience",
      "Modelling",
      "Spiking",
      "Multimodal"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://doi.org/10.1371/journal.pcbi.1013125"
      ],
      [
        "Preprint (biorxiv)",
        "https://www.biorxiv.org/content/10.1101/2024.12.19.629348v2"
      ],
      [
        "Preprint (HTML)",
        "https://www.biorxiv.org/content/10.1101/2024.12.19.629348v2.full"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/2024.12.19.629348v2.full.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/swathianil/Temporal_Nonlinear_fusion"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3lfpcgulazs2y"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/113826859929010069"
      ]
    ],
    "abstract": "Animals continuously combine information across sensory modalities and time, and use these combined signals to guide their behaviour. Picture a predator watching their prey sprint and screech through a field. To date, a range of multisensory algorithms have been proposed to model this process including linear and nonlinear fusion, which combine the inputs from multiple sensory channels via either a sum or nonlinear function. However, many multisensory algorithms treat successive observations independently, and so cannot leverage the temporal structure inherent to naturalistic stimuli. To investigate this, we introduce a novel multisensory task in which we provide the same number of task-relevant signals per trial but vary how this information is presented: from many short bursts to a few long sequences. We demonstrate that multisensory algorithms that treat different time steps as independent, perform sub-optimally on this task. However, simply augmenting these algorithms to integrate across sensory channels and short temporal windows allows them to perform surprisingly well, and comparably to fully recurrent neural networks. Overall, our work: highlights the benefits of fusing multisensory information across channels and time, shows that small increases in circuit/model complexity can lead to significant gains in performance, and provides a novel multisensory task for testing the relevance of this in biological systems.",
    "bluesky_thread": [
      "https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3lfpcgulazs2y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgw3pss2y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgwh4322y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgx32qk2y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgxicl22y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgxup2k2y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgyders2y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpcgypcmk2y",
      "https://bsky.app/profile/neural-reckoning.org/post/3lfpclblmk22y"
    ],
    "related": [
      "multimodal"
    ]
  },
  "path_to_universal_nca": {
    "year": 2025,
    "last_updated": "11-08-2025",
    "authors": "Béna G, Faldor M, Goodman DFM, Cully A",
    "title": "A Path to Universal Neural Cellular Automata",
    "conference": "GECCO",
    "additional_detail": "Proceedings of the Genetic and Evolutionary Computation Conference Companion",
    "categories": [
      "Neuroscience",
      "Machine learning",
      "Learning",
      "Modelling"
    ],
    "urls": [
      [
        "Conference (abstract)",
        "https://dl.acm.org/doi/10.1145/3712255.3734310"
      ],
      [
        "Conference (PDF)",
        "https://dl.acm.org/doi/pdf/10.1145/3712255.3734310"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/2505.13058"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2505.13058"
      ],
      [
        "Blog (HTML)",
        "https://gabrielbena.github.io/blog/2025/bena2025unca/"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsghrbb322i"
      ]
    ],
    "abstract": "Cellular automata have long been celebrated for their ability to generate complex behaviors from simple, local rules, with well-known discrete models like Conway's Game of Life proven capable of universal computation. Recent advancements have extended cellular automata into continuous domains, raising the question of whether these systems retain the capacity for universal computation. In parallel, neural cellular automata have emerged as a powerful paradigm where rules are learned via gradient descent rather than manually designed. This work explores the potential of neural cellular automata to develop a continuous Universal Cellular Automaton through training by gradient descent. We introduce a cellular automaton model, objective functions and training strategies to guide neural cellular automata toward universal computation in a continuous setting. Our experiments demonstrate the successful training of fundamental computational primitives - such as matrix multiplication and transposition - culminating in the emulation of a neural network solving the MNIST digit classification task directly within the cellular automata state. These results represent a foundational step toward realizing analog general-purpose computers, with implications for understanding universal computation in continuous dynamics and advancing the automated discovery of complex cellular automata behaviors via machine learning. ",
    "bluesky_thread": [
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsghrbb322i",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsghrbjuc2i",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsghrbktk2i",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsght3hes2i",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsgikjak22i",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsgikjbjc2i",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6gwuuk2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6hajhk2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6hddck2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6pgahk2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6pggd22f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6soxzk2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6spbs22f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6w6kdk2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6w6lcs2f",
      "https://bsky.app/profile/solarpunkgabs.bsky.social/post/3lqsh6woe722f"
    ]
  },
  "sparsity_specialization": {
    "selected": true,
    "year": 2025,
    "last_updated": "02-01-2025",
    "authors": "Béna G, Goodman DFM",
    "title": "Dynamics of specialization in neural modules under resource constraints",
    "journal": "Nature Communications",
    "doi": "10.1038/s41467-024-55188-9",
    "additional": "16, 187",
    "categories": [
      "Neuroscience",
      "Learning",
      "Modelling",
      "Machine learning"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://www.nature.com/articles/s41467-024-55188-9"
      ],
      [
        "Journal (PDF)",
        "https://www.nature.com/articles/s41467-024-55188-9.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/GabrielBena/specialization-dynamics"
      ],
      [
        "Data (figshare)",
        "https://doi.org/10.6084/m9.figshare.27161427"
      ],
      [
        "Preprint",
        "https://arxiv.org/abs/2106.02626"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2106.02626"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/neural-reckoning.org/post/3lggcx2k2522f"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/113878770587575972"
      ]
    ],
    "abstract": "The brain is structurally and functionally modular, although recent evidence has raised questions about the extent of both types of modularity. Using a simple, toy artificial neural network setup that allows for precise control, we find that structural modularity does not in general guarantee functional specialization (across multiple measures of specialization). Further, in this setup (1) specialization only emerges when features of the environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar across several different variations of network architectures. Finally, we show that functional specialization varies dynamically across time, and these dynamics depend on both the timing and bandwidth of information flow in the network. We conclude that a static notion of specialization is likely too simple a framework for understanding intelligence in situations of real-world complexity, from biology to brain-inspired neuromorphic systems.",
    "bluesky_thread": [
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx2k2522f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx6hmos2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx6hno22f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx6hno32f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx6hond2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx7dai32f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx7dbhd2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx7dcgl2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcx7ddft2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxajsu32f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxajusl2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxbeiel2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxbeiem2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxbenau2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxcn27u2f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxcn3742f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxcn3752f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxcn3762f",
      "https://bsky.app/profile/neural-reckoning.org/post/3lggcxcn46g2f"
    ]
  },
  "spytorch_transmitter": {
    "year": 2025,
    "last_updated": "17-09-2025",
    "authors": "Goodman D",
    "title": "This paper changed my life: Dan Goodman on a paper that reignited the field of spiking neural networks",
    "journal": "The Transmitter",
    "categories": [
      "Neuroscience",
      "Spiking",
      "Learning",
      "Machine learning",
      "Neuroinformatics",
      "Neural simulation"
    ],
    "urls": [
      [
        "Journal (HTML)",
        "https://www.thetransmitter.org/this-paper-changed-my-life/this-paper-changed-my-life-dan-goodman-on-a-paper-that-reignited-the-field-of-spiking-neural-networks/"
      ]
    ],
    "abstract": "Friedemann Zenke’s 2019 paper, and its related coding tutorial SpyTorch, made it possible to apply modern machine learning to spiking neural networks. The innovation reinvigorated the field.",
    "peer_reviewed": false
  },
  "beyond_rate_coding": {
    "year": "Preprints",
    "last_updated": "21-07-2025",
    "authors": "Yu Z, Sun P, Goodman DFM",
    "title": "Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks",
    "categories": [
      "Neuroscience",
      "Spiking",
      "Machine learning",
      "Auditory",
      "Neuroinformatics"
    ],
    "urls": [
      [
        "Preprint",
        "https://arxiv.org/abs/2507.16043"
      ],
      [
        "Preprint (PDF)",
        "https://arxiv.org/pdf/2507.16043"
      ],
      [
        "Code (GitHub)",
        "https://github.com/neural-reckoning/temporal-shd"
      ],
      [
        "Data (Zenodo)",
        "https://zenodo.org/records/16153275"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/neural-reckoning.org/post/3lupzbfiakk2m"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/114909302481345141"
      ]
    ],
    "abstract": "We investigate the extent to which Spiking Neural Networks (SNNs) trained with Surrogate Gradient Descent (Surrogate GD), with and without delay learning, can learn from precise spike timing beyond firing rates. We first design synthetic tasks isolating intra-neuron inter-spike intervals and cross-neuron synchrony under matched spike counts. On more complex spike-based speech recognition datasets (Spiking Heidelberg Digits (SHD) and Spiking Speech Commands (SSC), we construct variants where spike count information is eliminated and only timing information remains, and show that Surrogate GD-trained SNNs are able to perform significantly above chance whereas purely rate-based models perform at chance level. We further evaluate robustness under biologically inspired perturbations -- including Gaussian jitter per spike or per-neuron, and spike deletion -- revealing consistent but perturbation-specific degradation. Networks show a sharp performance drop when spike sequences are reversed in time, with a larger drop in performance from SNNs trained with delays, indicating that these networks are more human-like in terms of behaviour. To facilitate further studies of temporal coding, we have released our modified SHD and SSC datasets.",
    "bluesky_thread": [
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbfiakk2m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbgxp4c2m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbjcio22m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbk3v622m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbld4zk2m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbnotus2m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzborwwc2m",
      "https://bsky.app/profile/neural-reckoning.org/post/3lupzbqqa3k2m"
    ]
  },
  "neuromodulation_enhances_sensory": {
    "year": "Preprints",
    "last_updated": "31-07-2025",
    "authors": [
      "AlKilany A",
      "Goodman DFM"
    ],
    "title": "Neuromodulation enhances dynamic sensory processing in spiking neural network models",
    "doi": null,
    "categories": [
      "Neuroscience",
      "Spiking",
      "Machine learning",
      "Modelling",
      "Auditory"
    ],
    "urls": [
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/2025.07.25.666748v1"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/2025.07.25.666748v1.full.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/abdalalkilani/NeuromodulationSNN"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/neural-reckoning.org/post/3lz4rihm2622e"
      ],
      [
        "Mastodon",
        "https://neuromatch.social/@neuralreckoning/115226255243849089"
      ]
    ],
    "abstract": "Neuromodulators allow circuits to dynamically change their biophysical properties in a context-sensitive way. In addition to their role in learning, neuromodulators have been suggested to play a role in sensory processing at relatively fast timescales (less than a second), although the precise mechanisms at play are still not well understood. To assess the potential computational role of neuromodulators in sensory processing, we added a simple but flexible model of neuromodulation to spiking neural networks. These networks were then trained - with methods from machine learning - to carry out challenging sensory processing tasks. We find that this addition leads to a dramatic improvement in sensory processing in every task and configuration we tested. In particular, we find that without explicitly training for this, it decreases reaction times, a role that has been discussed for the cholinergic system. In a particularly challenging speech recognition in noise task, we find that the networks learn to make use of rapid dynamic gain control via excitability, an attentional mechanism akin to the “listening in the dips” strategy. This has been hypothesised to be a key element of human hearing allowing us to perform better in these conditions than even state-of-the-art machine learning systems. We conclude that neuromodulation does have the potential to play a significant computational role in fast sensory processing. In addition, our neuromodulated spiking neural networks are able to substantially increase performance at only a small cost to computational complexity, and may therefore be valuable for applications in energy-efficient “neuromorphic” computing devices.\n",
    "bluesky_thread": [
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4rihm2622e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4rijevis2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4rily4lk2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4rimo7mk2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4ringzks2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4riowvs22e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4ripmvvc2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4riqzybk2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4rituomk2e",
      "https://bsky.app/profile/neural-reckoning.org/post/3lz4riuhfas2e"
    ]
  },
  "partial_recurrence": {
    "year": "Preprints",
    "last_updated": "31-07-2025",
    "authors": [
      "Ghosh M",
      "Goodman DFM"
    ],
    "title": "Partial recurrence enables robust and efficient computation",
    "doi": null,
    "categories": [
      "Neuroscience",
      "Multimodal",
      "Machine learning",
      "Modelling"
    ],
    "urls": [
      [
        "Preprint",
        "https://www.biorxiv.org/content/10.1101/2025.07.28.667142v1"
      ],
      [
        "Preprint (PDF)",
        "https://www.biorxiv.org/content/10.1101/2025.07.28.667142v1.full.pdf"
      ],
      [
        "Code (GitHub)",
        "https://github.com/ghoshm/Multimodal_mazes"
      ],
      [
        "Bluesky",
        "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4r4urk26"
      ]
    ],
    "abstract": "Neural circuits are sparse and bidirectional. Meaning that signals flow from early sensory areas to later regions and back. Yet, between connected areas there exist some but not all pathways. How does this structure, somewhere between feedforward and fully recurrent, shape circuit function? To address this question, we designed a new recurrent neural network model in which a set of weight matrices (i.e. pathways) can be combined to generate every network structure between feedforward and fully recurrent. We term these architectures partially recurrent neural networks (pRNNs). We trained over 25,000 pRNNs on a novel set of reinforcement learning tasks, designed to mimic multisensory navigation, and compared their performance across multiple functional metrics. Our findings reveal three key insights. First, many architectures match or exceed the performance of fully recurrent networks, despite using as few as one-quarter the number of parameters; demonstrating that partial recurrence enables energy efficient, yet performant solutions. Second, each pathway’s functional impact is both task and circuit dependent. For instance, feedback connections enhance robustness to noise in some, but not all contexts. Third, different pRNN architectures learn solutions with distinct input sensitivities and memory dynamics, and these computational traits help to explain their functional capabilities. Overall, our results demonstrate that partial recurrence enables robust and efficient computation - a finding that helps to explain why neural circuits are sparse and bidirectional, and how these principles could inform the design of artificial systems.\n",
    "bluesky_thread": [
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4r4urk26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4uh37k26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4vg6dc26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4xl3fs26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4zjcmk26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda4zjdls26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda52ipis26",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda53ewt226",
      "https://bsky.app/profile/marcusghosh.bsky.social/post/3lvda54eark26"
    ]
  }
}