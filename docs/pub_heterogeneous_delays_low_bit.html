<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
    <META http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />

    <!-- social media stuff -->
    <meta name="twitter:widgets:csp" content="on">
    <meta name="twitter:dnt" content="on">
    <meta name="description" content="Neural networks rely on learning synaptic weights. However, this overlooks other neural parameters that can also be learned and may be utilized by the brain. On...">
    <meta property="og:url" content="https://neural-reckoning.org/pub_heterogeneous_delays_low_bit.html">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Exploiting heterogeneous delays for efficient computation in low-bit neural networks">
    <meta property="og:description" content="Neural networks rely on learning synaptic weights. However, this overlooks other neural parameters that can also be learned and may be utilized by the brain. On...">
    <meta property="og:image" content="https://neural-reckoning.org/default-social-media-card.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="neural-reckoning.org">
    <meta property="twitter:url" content="https://neural-reckoning.org/pub_heterogeneous_delays_low_bit.html">
    <meta name="twitter:title" content="Exploiting heterogeneous delays for efficient computation in low-bit neural networks">
    <meta name="twitter:description" content="Neural networks rely on learning synaptic weights. However, this overlooks other neural parameters that can also be learned and may be utilized by the brain. On...">
    <meta name="twitter:image" content="https://neural-reckoning.org/default-social-media-card.png">
    <meta name="fediverse:creator" content="@neuralreckoning@neuromatch.social"/>
    <!-- bsky embed feed -->
    <script type="module" src="https://cdn.jsdelivr.net/npm/bsky-embed/dist/bsky-embed.es.js" async></script>


    <title>Exploiting heterogeneous delays for efficient computation in low-bit neural networks</title>
    <link rel="STYLESHEET" href="style.css" type="text/css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">

</head>
<body>

<nav class="navbar sticky-top navbar-expand-xl navbar-light bg-light"><div class="container">
    <a class="navbar-brand" href="index.html">
            <img src="nr-logo-small.png" class="img-fluid" style="width: 4em;">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
                <li class="nav item">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle" href="#" id="navbarMembersDropdown" role="button"
                       data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">People</a>
        			<div class="dropdown-menu" aria-labelledby="navbarMembersDropdown">
                        <a class="dropdown-item" href="members.html">Everyone</a>
                        <div class="dropdown-divider"></div>
                                    <a class="dropdown-item" href="dan_goodman.html">Dan Goodman</a>
                                <h6 class="dropdown-header">Postdocs and Fellows</h6>
                                    <a class="dropdown-item" href="chu_yang.html">Yang Chu</a>
                                    <a class="dropdown-item" href="danyal_akarca.html">Danyal Akarca</a>
                                    <a class="dropdown-item" href="hardik_rajpal.html">Hardik Rajpal</a>
                                    <a class="dropdown-item" href="pengfei_sun.html">Pengfei Sun</a>
                                <h6 class="dropdown-header">PhD students</h6>
                                    <a class="dropdown-item" href="gabriel_bena.html">Gabriel BÃ©na</a>
                                    <a class="dropdown-item" href="greta_horvathova.html">Greta Horvathova</a>
                                    <a class="dropdown-item" href="jatin_sharma.html">Jatin Sharma</a>
                                    <a class="dropdown-item" href="abdelqader_alkilany.html">AbdelQader AlKilany</a>
                                <h6 class="dropdown-header">Affiliated members</h6>
                                    <a class="dropdown-item" href="marcus_ghosh.html">Marcus Ghosh</a>
                    </div>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="publications.html">Publications</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="software.html">Software</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="themes.html">Themes</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="openings.html">Join us</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="location.html">Location</a>
                </li>
            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="navbarMiscDropdown" role="button"
                   data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
                <div class="dropdown-menu" aria-labelledby="navbarMiscDropdown">
                        <a class="dropdown-item" href="videos.html">Videos</a>
                        <a class="dropdown-item" href="organisations.html">Organisations</a>
                        <a class="dropdown-item" href="comp-neuro-resources.html">Computational neuroscience resources</a>
                        <a class="dropdown-item" href="reviewing.html">Ending support for legacy academic publishing</a>
                        <a class="dropdown-item" href="neuroinformatics.html">Neuroinformatics</a>
                        <a class="dropdown-item" href="sensory.html">Sensory neuroscience</a>
                        <a class="dropdown-item" href="mathematics.html">Mathematics</a>
                        <a class="dropdown-item" href="apply_phd.html">PhD application process</a>
                        <a class="dropdown-item" href="accessibility.html">Accessibility statement</a>
                </div>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://neuromatch.social/@neuralreckoning" rel="me"><i class="fa-brands fa-mastodon"></i><span class="d-inline d-xl-none"> Mastodon</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://bsky.app/profile/neural-reckoning.org" rel="me"><i class="fa-brands fa-bluesky"></i><span class="d-inline d-xl-none"> Bluesky</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://twitter.com/neuralreckoning" rel="me"><i class="fa-brands fa-twitter"></i><span class="d-inline d-xl-none"> Twitter</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://github.com/neural-reckoning" rel="me"><i class="fa-brands fa-github"></i><span class="d-inline d-xl-none"> GitHub</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://www.youtube.com/@neuralreckoning" rel="me"><i class="fa-brands fa-youtube"></i><span class="d-inline d-xl-none"> YouTube</span></a>
            </li>
        </ul>
    </div>
</div></nav>

<!-- <div class="hideAfterDeadline alert alert-success" data-deadline="2024-10-01" style="text-align: center">
    <a href="https://www.imperial.ac.uk/jobs/search-jobs/description/index.php?jobId=20479&jobTitle=Research+Associate+in+Computational+Neuroscience%2FNeuroAI%2FNeuromorphic+Systems" style="color: black">We have a postdoctoral position available (deadline Sept 30). Click for details.</a>
</div> -->

<div class="container">

    <p>&nbsp;</p>

<div class="main">

<div class="row">
    <div class="col-lg-4 d-none d-lg-block order-lg-12">
        <div style="height: 90vh; overflow-y: scroll;">
            
            <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpksani22m" data-bluesky-cid="bafyreihkprtsup6jkwjma5quwd63puuhy35tswt7cow4pegmtjj4emhywe"><p lang="en">Psst - neuromorphic folks. Did you know that you can solve the SHD dataset with 90% accuracy using only 22 kb of parameter memory by quantising weights and delays? Check out our preprint with @pengfei-sun.bsky.social and @danakarca.bsky.social, or read the TLDR below. ðŸ‘‡ðŸ¤–ðŸ§ ðŸ§ª arxiv.org/abs/2510.27434</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpksani22m?ref_src=embed">2025-11-13T17:40:46.232Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpktqxes2m" data-bluesky-cid="bafyreiggiojtr336s7nyg22igi72hjnm3ctuvscig6nev6z4lsyv6dubaa"><p lang="en">We&#39;ve known for a while now that adding learnable delays is a great way to improve performance for this and other temporal datasets. Just check out how often the word &#39;delay&#39; crops up on @fzenke.bsky.social&#39;s leaderboard for the top performers at SHD.</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpktqxes2m?ref_src=embed">2025-11-13T17:40:46.233Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpkvomz22m" data-bluesky-cid="bafyreia3cq73f6h4lkkykelhhgljy23vcjwcpvloqjz7ydpxz74y55q5ci"><p lang="en">Most people (including us) are using axonal delays so the number of parameters scales well as you increase network size - O(n) for axonal delays compared to O(nÂ²) for weights. You get a lot more bang for your buck if you add in delays.</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpkvomz22m?ref_src=embed">2025-11-13T17:40:46.234Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpkwscmc2m" data-bluesky-cid="bafyreiencicllo4k5mhcngyuzn4k7kmvwwo4i2hvmttbyzx2qiebhnsiyq"><p lang="en">So how small (in terms of memory usage) could you make your network and keep good performance? We used learnable quantisations of both weights and delays using different bit budgets, and checked performance. Here&#39;s SHD. The point marked I has almost optimal performance but uses hardly any bits.</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpkwscmc2m?ref_src=embed">2025-11-13T17:40:46.235Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpkxsxjs2m" data-bluesky-cid="bafyreiesmerisdlfldzqfq5n6fu64q4ujwoxa5mtaenmmt6gemmtoysy7y"><p lang="en">Optimal performance is at point II on the graph above, with 4 bit weights and 5 bit delays gives 94% accuracy - only 2% off the best known performance on this dataset using only 4% of the memory footprint.

Or with 1.58 bit weights and 3 bit delays we get 90% accuracy using only 22 kB memory.</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpkxsxjs2m?ref_src=embed">2025-11-13T17:40:46.236Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpkxsyj22m" data-bluesky-cid="bafyreifnw3hvxwuqktilwbvi4p5nq6w4p2rdkfsxvzkdwyxdqaqifni5oq"><p lang="en">Just as a note here, the 1.58 bits for weights is because weights can take on three values, a positive, negative, or zero weight. So that&#39;s logâ‚‚3=1.58 bits per weight. Depending on sparsity level, a sparse matrix format might be able to use even fewer bits on average.</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpkxsyj22m?ref_src=embed">2025-11-13T17:40:46.237Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpkxt4g22m" data-bluesky-cid="bafyreiexorqumkgiahiz7eqac2alzhsbhrrn5n67qfcr7cgayrzvjp4po4"><p lang="en">What we find is that the most important delays are the long ones. Selectively pruning the longest delays leads to a sharper fall in performance than the shorter ones. This is actually a problem for devices where long delays are expensive. We can reduce this with regularisation, but more work needed.</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpkxt4g22m?ref_src=embed">2025-11-13T17:40:46.238Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:niqde7rkzo7ua3scet2rzyt7/app.bsky.feed.post/3m5jpkynk4k2m" data-bluesky-cid="bafyreienihoeh76nzn353xqfujyj6ndmcshi5ext7t665hgfrx7vianbse"><p lang="en">So here&#39;s our challenge to the community: how few bits do you need to solve the SHD dataset at 90%&#43; accuracy? Let us know if you can beat 22 kB. Just to motivate you, here&#39;s something you can do with just 16 kB of code:

youtu.be/oITx9xMrAcM?...

Time to bring the PC demoscene spirit to SNNs?</p>&mdash; <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7?ref_src=embed">Dan Goodman (@neural-reckoning.org)</a> <a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpkynk4k2m?ref_src=embed">2025-11-13T17:40:46.239Z</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
        </div>
    </div>
    <div class="col-lg-8 order-lg-1">

<h2>Exploiting heterogeneous delays for efficient computation in low-bit neural networks</h2>
    <div class="d-none d-sm-block">
        <ul class="list-inline author-list">
    <li>
        <a href="pengfei_sun.html">
        <div><img src="photo_pengfei_sun.s.circ.png"/></div>
        <div>Pengfei Sun</div>
        </a>
    </li>
    <li>
        <div><img src="photo_placeholder.s.circ.png"/></div>
        <div>Achterberg J</div>
    </li>
    <li>
        <div><img src="photo_placeholder.s.circ.png"/></div>
        <div>Su Z</div>
    </li>
    <li>
        <a href="dan_goodman.html">
        <div><img src="photo_dan_goodman.s.circ.png"/></div>
        <div>Dan Goodman</div>
        </a>
    </li>
    <li>
        <a href="danyal_akarca.html">
        <div><img src="photo_danyal_akarca.s.circ.png"/></div>
        <div>Danyal Akarca</div>
        </a>
    </li>
</ul>

    </div>
    <div class="d-block d-sm-none">
        <a href="pengfei_sun.html">Sun P</a>, Achterberg J, Su Z, <a href="dan_goodman.html">Goodman DFM</a>, <a href="danyal_akarca.html">Akarca D</a>
    </div>
<div><i>Preprint</i></div>
<div>&nbsp;</div>
<h3>Abstract</h3>
<div>Neural networks rely on learning synaptic weights. However, this overlooks other neural parameters that can also be learned and may be utilized by the brain. One such parameter is the delay: the brain exhibits complex temporal dynamics with heterogeneous delays, where signals are transmitted asynchronously between neurons. It has been theorized that this delay heterogeneity, rather than a cost to be minimized, can be exploited in embodied contexts where task-relevant information naturally sits contextually in the time domain. We test this hypothesis by training spiking neural networks to modify not only their weights but also their delays at different levels of precision. We find that delay heterogeneity enables state-of-the-art performance on temporally complex neuromorphic problems and can be achieved even when weights are extremely imprecise (1.58-bit ternary precision: just positive, negative, or absent). By enabling high performance with extremely low-precision weights, delay heterogeneity allows memory-efficient solutions that maintain state-of-the-art accuracy even when weights are compressed over an order of magnitude more aggressively than typically studied weight-only networks. We show how delays and time-constants adaptively trade-off, and reveal through ablation that task performance depends on task-appropriate delay distributions, with temporally-complex tasks requiring longer delays. Our results suggest temporal heterogeneity is an important principle for efficient computation, particularly when task-relevant information is temporal - as in the physical world - with implications for embodied intelligent systems and neuromorphic hardware.</div>

        <h3 class="pt-3">Links</h3>
        <p>
<button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://arxiv.org/abs/2510.27434">Preprint</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://arxiv.org/pdf/2510.27434"><i class="fa-regular fa-file-pdf"></i> Preprint (PDF)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://bsky.app/profile/did:plc:niqde7rkzo7ua3scet2rzyt7/post/3m5jpksani22m"><i class="fa-brands fa-bluesky"></i> Bluesky</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://x.com/neuralreckoning/status/1989027151522259378"><i class="fa-brands fa-twitter"></i> Twitter</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://neuromatch.social/@neuralreckoning/115543629248037178"><i class="fa-brands fa-mastodon"></i> Mastodon</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://www.linkedin.com/posts/dan-goodman-78658b24a_exploiting-heterogeneous-delays-for-efficient-activity-7394793296428105728-evZK">LinkedIn</a></button>        </p>
    <h3 class="pt-3">Categories</h3>
    <p>
<button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_machinelearning.html">Machine learning</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_neuroinformatics.html">Neuroinformatics</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_neuroscience.html">Neuroscience</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_software.html">Software</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_spiking.html">Spiking</a></button>    </p>



    <p>&nbsp;</p>

</div>

<!-- Optional JavaScript for Bootstrap -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>

<script type="module">
    import { thingsToDoOnLoad } from './nr.js';
    thingsToDoOnLoad();
</script>

</body>
</html>