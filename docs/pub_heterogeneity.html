<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
    <META http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />

    <!-- social media stuff -->
    <meta name="description" content="The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been r...">
    <meta property="og:url" content="https://neural-reckoning.org/pub_heterogeneity.html">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Neural heterogeneity promotes robust learning">
    <meta property="og:description" content="The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been r...">
    <meta property="og:image" content="https://neural-reckoning.org/default-social-media-card.png">
    <meta name="fediverse:creator" content="@neuralreckoning@neuromatch.social"/>
    <!-- bsky embed feed -->
    <script type="module" src="https://cdn.jsdelivr.net/npm/bsky-embed/dist/bsky-embed.es.js" async></script>


    <title>Neural heterogeneity promotes robust learning</title>
    <link rel="STYLESHEET" href="style.css" type="text/css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">

</head>
<body>

<nav class="navbar sticky-top navbar-expand-xl navbar-light bg-light"><div class="container">
    <a class="navbar-brand" href="index.html">
            <img src="nr-logo-small.png" class="img-fluid" style="width: 4em;">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
                <li class="nav item">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle" href="#" id="navbarMembersDropdown" role="button"
                       data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">People</a>
        			<div class="dropdown-menu" aria-labelledby="navbarMembersDropdown">
                        <a class="dropdown-item" href="members.html">Everyone</a>
                        <div class="dropdown-divider"></div>
                                    <a class="dropdown-item" href="dan_goodman.html">Dan Goodman</a>
                                <h6 class="dropdown-header">Postdocs and Fellows</h6>
                                    <a class="dropdown-item" href="chu_yang.html">Yang Chu</a>
                                    <a class="dropdown-item" href="danyal_akarca.html">Danyal Akarca</a>
                                    <a class="dropdown-item" href="hardik_rajpal.html">Hardik Rajpal</a>
                                    <a class="dropdown-item" href="pengfei_sun.html">Pengfei Sun</a>
                                <h6 class="dropdown-header">PhD students</h6>
                                    <a class="dropdown-item" href="gabriel_bena.html">Gabriel Béna</a>
                                    <a class="dropdown-item" href="greta_horvathova.html">Greta Horvathova</a>
                                    <a class="dropdown-item" href="jatin_sharma.html">Jatin Sharma</a>
                                    <a class="dropdown-item" href="abdelqader_alkilany.html">AbdelQader AlKilany</a>
                                    <a class="dropdown-item" href="nicolas_dundov.html">Nicolás Dundov Muñoz</a>
                                <h6 class="dropdown-header">Affiliated members</h6>
                                    <a class="dropdown-item" href="marcus_ghosh.html">Marcus Ghosh</a>
                    </div>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="publications.html">Publications</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="software.html">Software</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="themes.html">Themes</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="openings.html">Join us</a>
                </li>
                <li class="nav item">
                    <a class="nav-link" href="location.html">Location</a>
                </li>
            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="navbarMiscDropdown" role="button"
                   data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
                <div class="dropdown-menu" aria-labelledby="navbarMiscDropdown">
                        <a class="dropdown-item" href="videos.html">Videos</a>
                        <a class="dropdown-item" href="organisations.html">Organisations</a>
                        <a class="dropdown-item" href="comp-neuro-resources.html">Computational neuroscience resources</a>
                        <a class="dropdown-item" href="reviewing.html">Ending support for legacy academic publishing</a>
                        <a class="dropdown-item" href="metascience.html">Metascience</a>
                        <a class="dropdown-item" href="neuroinformatics.html">Neuroinformatics</a>
                        <a class="dropdown-item" href="sensory.html">Sensory neuroscience</a>
                        <a class="dropdown-item" href="mathematics.html">Mathematics</a>
                        <a class="dropdown-item" href="apply_phd.html">PhD application process</a>
                        <a class="dropdown-item" href="accessibility.html">Accessibility statement</a>
                </div>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://neuromatch.social/@neuralreckoning" rel="me"><i class="fa-brands fa-mastodon"></i><span class="d-inline d-xl-none"> Mastodon</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://bsky.app/profile/neural-reckoning.org" rel="me"><i class="fa-brands fa-bluesky"></i><span class="d-inline d-xl-none"> Bluesky</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://github.com/neural-reckoning" rel="me"><i class="fa-brands fa-github"></i><span class="d-inline d-xl-none"> GitHub</span></a>
            </li>
            <li class="nav item">
                <a class="nav-link" href="https://www.youtube.com/@neuralreckoning" rel="me"><i class="fa-brands fa-youtube"></i><span class="d-inline d-xl-none"> YouTube</span></a>
            </li>
        </ul>
    </div>
</div></nav>

<!-- <div class="hideAfterDeadline alert alert-success" data-deadline="2024-10-01" style="text-align: center">
    <a href="https://www.imperial.ac.uk/jobs/search-jobs/description/index.php?jobId=20479&jobTitle=Research+Associate+in+Computational+Neuroscience%2FNeuroAI%2FNeuromorphic+Systems" style="color: black">We have a postdoctoral position available (deadline Sept 30). Click for details.</a>
</div> -->

<div class="container">

    <p>&nbsp;</p>

<div class="main">

<div class="row">
    <div class="col-lg-4 d-none d-lg-block order-lg-12">
        <div><!--</div> style="height: 90vh; overflow-y: scroll;">-->
            
            <div class="explainer_html smaller_text">
                <h5>The short version</h5>
                Have you ever wondered why neurons are like snowflakes? No two alike, even if they're the same type. In our latest preprint, we think we have (at least part of) the answer: it promotes robust learning. <img src="https://pbs.twimg.com/media/EpwgTeQWwAAOqgN?format=jpg&name=small"/> <p> One of the striking things about the brain is how much diversity there is at so many levels, e.g. the distribution of membrane time constants. Check these out from the Allen Institute and Paul Manis - there's a wide range of values for single cell types, a bit like a log normal distribution. <img src="https://pbs.twimg.com/media/EpwiOS2WwAEbp-a?format=jpg&name=small"/> <p> But, most models of networks of neurons that can carry out complex information processing tasks tend to use the same parameters for all neurons of the same type, with typically only connectivity being different for each neuron. <p> We guessed that networks of neurons with a wide range of time constants would be better at tasks where information is present at multiple time scales, such as auditory tasks like recognising speech. <p> Since we were interested in temporal dynamics and comparing to biology, we used spiking neural networks trained using <a href="https://github.com/fzenke/spytorch">surrogate gradient descent</a>, adapted to learn time constants as well as weights. <p> We found no improvement for N-MNIST which has little to no useful temporal information in the stimuli, some improvement for DVS gestures which does have temporal info but can be solved well without using it, and a huge improvement for recognising spoken digits. <img src="https://pbs.twimg.com/media/EpwoXCUWwAAFjPn?format=jpg&name=small"/> <p> The distributions of time constants learned are consistent for a given task, for each run, and regardless of whether you initialise time constants randomly or at a fixed value. And, they look like the distributions you find in the brain. <img src="https://pbs.twimg.com/media/Epwp1TgWMAAaNEo?format=jpg&name=small"/> <p> And it's more robust. If you tune hyperparameters for sounds at a single speed, and then change the playback speed of stimuli, the heterogeneous networks can still learn the task but homogeneous ones start to fall down. <img src="https://pbs.twimg.com/media/Epwz4SiXcAENGkL?format=jpg&name=small"/> <p> We also tested another training method, spiking FORCE <a href="https://doi.org/10.1038/s41467-017-01827-3">(Nicola and Clopath 2017)</a>, and found the same robustness to hyperparameter mistuning. This figure shows the region where learning converges to a good solution in blue, axes are hyperparameters. <img src="https://pbs.twimg.com/media/Epw1yK9XIAYEJRt?format=jpg&name=small"/> <p> So this looks to be pretty general: heterogeneity improves learning of temporal tasks and gives robustness against hyperparameter mistuning. And it does so at no metabolic cost. The same performance with homogeneous networks requires 10x more neurons! So, surely the brain uses this? <p> This is also good for neuromorphic computing and ML: adding neuron level heterogeneity costs only O(n) time and memory, whereas adding more neurons and synapses costs O(n²). <p> That's it for the results, we hope that this spurs people to look further into the importance of heterogeneity, e.g. spatial or cell type heterogeneity, and whether it can be useful in ML too. <p> If you're interested in this area, there's a fascinating literature on heterogeneity, briefly reviewed in this paper, including similar work with a more neuromorphic angle from Sander Bohte and Tim Masquelier. There's a nice review in <a href="https://doi.org/10.1016/j.conb.2015.12.008">Gjorgjieva et al. (2016)</a>. <p> This work was only possible thanks to two incredible scientific developments. The first is new methods of training spiking neural networks from Friedemann Zenke and others. We had a workshop on this over the summer, videos available at the <a href="https://www.youtube.com/playlist?list=PL09WqqDbQWHFvM9DFYkM_GfnrVnIdLRhy">SNUFA 2020 playlist</a>. <p> The second is the availability of incredible experimental datasets thanks to orgs like the Allen Institute, the <a href="https://neuroelectro.org">NeuroElectro Database</a> and individual labs like Paul Manis'. Thank you so much for your generosity!
            </div>
        </div>
    </div>
    <div class="col-lg-8 order-lg-1">

<h2>Neural heterogeneity promotes robust learning</h2>
    <div class="d-none d-sm-block">
        <ul class="list-inline author-list">
    <li>
        <a href="nicolas_perez.html">
        <div><img src="photo_nicolas_perez.s.circ.png"/></div>
        <div>Nicolas Perez</div>
        </a>
    </li>
    <li>
        <div><img src="photo_placeholder.s.circ.png"/></div>
        <div>Leung VCH</div>
    </li>
    <li>
        <div><img src="photo_placeholder.s.circ.png"/></div>
        <div>Dragotti PL</div>
    </li>
    <li>
        <a href="dan_goodman.html">
        <div><img src="photo_dan_goodman.s.circ.png"/></div>
        <div>Dan Goodman</div>
        </a>
    </li>
</ul>

    </div>
    <div class="d-block d-sm-none">
        <a href="nicolas_perez.html">Perez-Nieves N</a>, Leung VCH, Dragotti PL, <a href="dan_goodman.html">Goodman DFM</a>
    </div>
<div>
    <i>Nature Communications</i>
    <i></i>
    <i></i>
    (2021) <font color="#aaaaaa">12, 5791</font>
</div>
<div>
    <a href="http://dx.doi.org/10.1038/s41467-021-26022-3" style="color: #aaaaaa">doi: 10.1038/s41467-021-26022-3</a>
</div>
<div>&nbsp;</div>
<h3>Abstract</h3>
<div>The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been relatively little explored in models which are often highly homogeneous. We compared the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that heterogeneity substantially improved task performance. Learning with heterogeneity was more stable and robust, particularly for tasks with a rich temporal structure. In addition, the distribution of neuronal parameters in the trained networks is similar to those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.</div>
<div>&nbsp;</div>
<div class="embed-responsive embed-responsive-16by9">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/aLNGcs_zYsY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

        <h3 class="pt-3">Links</h3>
        <p>
<button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://www.nature.com/articles/s41467-021-26022-3"><i class="fa-regular fa-file-lines"></i> Journal (HTML)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://www.nature.com/articles/s41467-021-26022-3.pdf"><i class="fa-regular fa-file-pdf"></i> Journal (PDF)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://www.biorxiv.org/content/10.1101/2020.12.18.423468v3">Preprint</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://github.com/npvoid/neural_heterogeneity">Code (GitHub)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://zenodo.org/record/5413181">Code (Zenodo)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://www.youtube.com/watch?v=V2HFqVfeTPg&feature=youtu.be"><i class="fa-solid fa-video"></i> Neurotheory talk by Dan Goodman (video)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://youtu.be/aLNGcs_zYsY"><i class="fa-solid fa-video"></i> SNUFA 2021 talk by Nicolas Perez (video)</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="https://open.spotify.com/episode/6PYxjRFg2bI8pSDN82Dume?si=S7Z4BxXNS2aRCnbBMskTHg">Podcast with WaterCooler Neuroscience</a></button>        </p>
        <h3 class="pt-5 pb-3">Related videos</h3>
            <div class="embed-responsive embed-responsive-16by9">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/aLNGcs_zYsY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div>
    <ul class="list-group">
        <li class="list-group-item d-flex justify-content-between align-items-center">
    <div>
        <a href="video_heterogeneity_snufa.html">Understanding the role of neural heterogeneity in learning</a><br/>
        Talk on neural heterogeneity by Nicolas Perez.    
    </div>
    <span>Talk<span style="color: #aaaaaa;"> / 2021</span></span>
</li>

        <li class="list-group-item d-flex justify-content-between align-items-center">
    <div>
        <a href="video_heterogeneity_neurotheory.html">Neural heterogeneity promotes robust learning</a><br/>
        Talk on neural heterogeneity by Dan Goodman.    
    </div>
    <span>Talk<span style="color: #aaaaaa;"> / 2021</span></span>
</li>

    </ul>
</div>


    <h3 class="pt-3">Categories</h3>
    <p>
<button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_auditory.html">Auditory</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_learning.html">Learning</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_machinelearning.html">Machine learning</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_modelling.html">Modelling</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_neuroscience.html">Neuroscience</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_sensory.html">Sensory</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_spiking.html">Spiking</a></button><button type="button" class="btn btn-light" style="margin: 2px;"><a href="publication_category_visual.html">Visual</a></button>    </p>


<!-- bootstrap only show div on small screens -->
<div class="d-block d-lg-none">
    <h3>The short version</h3>
    
    <div class="explainer_html">
        Have you ever wondered why neurons are like snowflakes? No two alike, even if they're the same type. In our latest preprint, we think we have (at least part of) the answer: it promotes robust learning. <img src="https://pbs.twimg.com/media/EpwgTeQWwAAOqgN?format=jpg&name=small"/> <p> One of the striking things about the brain is how much diversity there is at so many levels, e.g. the distribution of membrane time constants. Check these out from the Allen Institute and Paul Manis - there's a wide range of values for single cell types, a bit like a log normal distribution. <img src="https://pbs.twimg.com/media/EpwiOS2WwAEbp-a?format=jpg&name=small"/> <p> But, most models of networks of neurons that can carry out complex information processing tasks tend to use the same parameters for all neurons of the same type, with typically only connectivity being different for each neuron. <p> We guessed that networks of neurons with a wide range of time constants would be better at tasks where information is present at multiple time scales, such as auditory tasks like recognising speech. <p> Since we were interested in temporal dynamics and comparing to biology, we used spiking neural networks trained using <a href="https://github.com/fzenke/spytorch">surrogate gradient descent</a>, adapted to learn time constants as well as weights. <p> We found no improvement for N-MNIST which has little to no useful temporal information in the stimuli, some improvement for DVS gestures which does have temporal info but can be solved well without using it, and a huge improvement for recognising spoken digits. <img src="https://pbs.twimg.com/media/EpwoXCUWwAAFjPn?format=jpg&name=small"/> <p> The distributions of time constants learned are consistent for a given task, for each run, and regardless of whether you initialise time constants randomly or at a fixed value. And, they look like the distributions you find in the brain. <img src="https://pbs.twimg.com/media/Epwp1TgWMAAaNEo?format=jpg&name=small"/> <p> And it's more robust. If you tune hyperparameters for sounds at a single speed, and then change the playback speed of stimuli, the heterogeneous networks can still learn the task but homogeneous ones start to fall down. <img src="https://pbs.twimg.com/media/Epwz4SiXcAENGkL?format=jpg&name=small"/> <p> We also tested another training method, spiking FORCE <a href="https://doi.org/10.1038/s41467-017-01827-3">(Nicola and Clopath 2017)</a>, and found the same robustness to hyperparameter mistuning. This figure shows the region where learning converges to a good solution in blue, axes are hyperparameters. <img src="https://pbs.twimg.com/media/Epw1yK9XIAYEJRt?format=jpg&name=small"/> <p> So this looks to be pretty general: heterogeneity improves learning of temporal tasks and gives robustness against hyperparameter mistuning. And it does so at no metabolic cost. The same performance with homogeneous networks requires 10x more neurons! So, surely the brain uses this? <p> This is also good for neuromorphic computing and ML: adding neuron level heterogeneity costs only O(n) time and memory, whereas adding more neurons and synapses costs O(n²). <p> That's it for the results, we hope that this spurs people to look further into the importance of heterogeneity, e.g. spatial or cell type heterogeneity, and whether it can be useful in ML too. <p> If you're interested in this area, there's a fascinating literature on heterogeneity, briefly reviewed in this paper, including similar work with a more neuromorphic angle from Sander Bohte and Tim Masquelier. There's a nice review in <a href="https://doi.org/10.1016/j.conb.2015.12.008">Gjorgjieva et al. (2016)</a>. <p> This work was only possible thanks to two incredible scientific developments. The first is new methods of training spiking neural networks from Friedemann Zenke and others. We had a workshop on this over the summer, videos available at the <a href="https://www.youtube.com/playlist?list=PL09WqqDbQWHFvM9DFYkM_GfnrVnIdLRhy">SNUFA 2020 playlist</a>. <p> The second is the availability of incredible experimental datasets thanks to orgs like the Allen Institute, the <a href="https://neuroelectro.org">NeuroElectro Database</a> and individual labs like Paul Manis'. Thank you so much for your generosity!
    </div>
</div>


    <p>&nbsp;</p>

</div>

<!-- Optional JavaScript for Bootstrap -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>

<script type="module">
    import { thingsToDoOnLoad } from './nr.js';
    thingsToDoOnLoad();
</script>

</body>
</html>