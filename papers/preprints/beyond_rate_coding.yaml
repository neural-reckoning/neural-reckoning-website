# selected: True
year: Preprints
last_updated: 18-12-2025
authors: Yu Z, Sun P, Goodman DFM
title: "Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks"
# journal: 
# Goes after Journal (Year)
# additional: 
# doi: 

categories:
  - Neuroscience
  - Spiking
  - Machine learning
  - Auditory
  - Neuroinformatics

urls:
  - - Preprint
    - https://arxiv.org/abs/2507.16043
  - - Preprint (PDF)
    - https://arxiv.org/pdf/2507.16043
  - - Code (GitHub)
    - https://github.com/neural-reckoning/temporal-shd
  - - Data (Zenodo)
    - https://zenodo.org/records/16153275
  - - Bluesky 
    - https://bsky.app/profile/neural-reckoning.org/post/3lupzbfiakk2m
  - - Mastodon
    - https://neuromatch.social/@neuralreckoning/114909302481345141

abstract: The surrogate gradient descent algorithm enabled spiking neural networks to be trained to carry out challenging sensory processing tasks, an important step in understanding how spikes contribute to neural computations. However, it is unclear the extent to which these algorithms fully explore the space of possible spiking solutions to problems. We investigated whether spiking networks trained with surrogate gradient descent can learn to make use of information that is only encoded in the timing and not the rate of spikes. We constructed synthetic datasets with a range of types of spike timing information (interspike intervals, spatio-temporal spike patterns or polychrony, and coincidence codes). We find that surrogate gradient descent training can extract all of these types of information. In more realistic speech-based datasets, both timing and rate information is present. We therefore constructed variants of these datasets in which all rate information is removed, and find that surrogate gradient descent can still perform well. We tested all networks both with and without trainable axonal delays. We find that delays can give a significant increase in performance, particularly for more challenging tasks. To determine what types of spike timing information are being used by the networks trained on the speech-based tasks, we test these networks on time-reversed spikes which perturb spatio-temporal spike patterns but leave interspike intervals and coincidence information unchanged. We find that when axonal delays are not used, networks perform well under time reversal, whereas networks trained with delays perform poorly. This suggests that spiking neural networks with delays are better able to exploit temporal structure. To facilitate further studies of temporal coding, we have released our modified speech-based datasets. 
bluesky_thread:
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbfiakk2m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbgxp4c2m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbjcio22m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbk3v622m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbld4zk2m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbnotus2m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzborwwc2m
- https://bsky.app/profile/neural-reckoning.org/post/3lupzbqqa3k2m