year: Preprints
last_updated: 22-05-2024
authors: BÃ©na G, Goodman DFM
title: Dynamics of specialization in neural modules under resource constraints
categories:
- Neuroscience
- Learning
- Modelling
- Machine learning
urls:
- - Preprint
  - https://arxiv.org/abs/2106.02626
- - Preprint (PDF)
  - https://arxiv.org/pdf/2106.02626
- - Mastodon
  - https://neuromatch.social/@neuralreckoning/110792288348408416
- - Twitter
  - https://twitter.com/neuralreckoning/status/1684936490608934914
abstract: It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that in this setup (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar across the different variations of network architectures that we tested, but that the quantitative relationships depend on the precise architecture. Finally, we show that functional specialization varies dynamically across time, and demonstrate that these dynamics depend on both the timing and bandwidth of information flow in the network. We conclude that a static notion of specialization, based on structural modularity, is likely too simple a framework for understanding intelligence in situations of real-world complexity, from biology to brain-inspired neuromorphic systems. We propose that thoroughly stress testing candidate definitions of functional modularity in simplified scenarios before extending to more complex data, network models and electrophysiological recordings is likely to be a fruitful approach. 
last_tweet_in_thread: '1405131238160179201'
twitter_thread: >
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/preprint?src=hash&amp;ref_src=twsrc%5Etfw">#preprint</a>! Defining neural modularity is hard: much history. We used toy ANNs to show structural and functional definitions not tightly related, resource constraints important, and we need to start thinking about temporal dynamics.<br><br>ðŸ§µ with <a href="https://twitter.com/GabrielBna1?ref_src=twsrc%5Etfw">@GabrielBna1</a> <a href="https://t.co/h70TXa7jFT">https://t.co/h70TXa7jFT</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936490608934914?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">To get at structural modularity we use the simplest possible setup, 2 modules with input, output and varying amounts of connections between modules. Highly modular when p close to 0, not modular at all when p close to 1. Modularity Q=(1-p)/2(1+p). <a href="https://t.co/4ikd3Twu6s">pic.twitter.com/4ikd3Twu6s</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936492592828417?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">Input consists of pairs of MNIST digits. Task is to return one or the other of the two digits depending on whether their parity is the same or different. If modules specialise on one digit, they only need to communicate a single parity bit. (Messy details: see paper.)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936495029731328?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">Getting at functional modularity is harder! We came up with three definitions that it turns out relate to ideas from Fodor, Shallice and Cooper. Either training decoders on frozen (and ablated) networks, or looking for correlations in module states. They give coherent results! ðŸ¥³ <a href="https://t.co/nz3dBQOWaL">pic.twitter.com/nz3dBQOWaL</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936496690741248?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">First result: tuning structural modularity Q is enough to induce functional modularity, but only at levels way beyond what is considered highly structurally modular (Q&gt;0.4). The task is simple but shows that brainlike structural modularity not enough to get functional modularity. <a href="https://t.co/Z2CaFATsOh">pic.twitter.com/Z2CaFATsOh</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936498943025152?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">If you vary number of neurons n, connectivity p, covariance in input data c and architecture, you get a more complicated picture. In brief, having tighter resource constraints and certain types of architecture make functional modularity more likely to emerge. <a href="https://t.co/imNHPcmKYH">pic.twitter.com/imNHPcmKYH</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936501086257153?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">Unrolling the recurrent network in time and treating different time points as different modules, we find that specialisation has rich temporal dynamics. In brief, specialisation dynamics follows the dynamics of information flow through the network (check out the paper for more). <a href="https://t.co/ipNCVKGWqI">pic.twitter.com/ipNCVKGWqI</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936503355396096?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">This is based on toy models. Good! If we can&#39;t define what we mean in the simplest possible case, what hope do we have for real world cases? We think toy ANNs are a great proving ground for candidate definitions of modularity, and that we need new ideas for dynamic modularity.</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936505649729536?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
  <blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">Once we have good working definitions for these simple cases, we can start applying them to more complex artificial and biological neural networks. The definitions in this paper could be used with NeuroPixels data for example.</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1684936507306471425?ref_src=twsrc%5Etfw">July 28, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 