year: Preprints
last_updated: 4-6-2021
authors: BÃ©na G, Goodman DFM
title: Extreme sparsity gives rise to functional specialization
categories:
- Neuroscience
- Learning
- Modelling
- Machine learning
urls:
- - Preprint
  - https://arxiv.org/abs/2106.02626
- - Preprint (PDF)
  - https://arxiv.org/pdf/2106.02626
- - Preprint (HTML)
  - https://neural-reckoning.github.io/preprints/sparsity-specialization/
- - Twitter
  - https://twitter.com/neuralreckoning/status/1405131198834282499
abstract: 'Modularity of neural networks â€“ both biological and artificial â€“ can be
  thought of either structurally or functionally, and the relationship between these
  is an open question. We show that enforcing structural modularity via sparse connectivity
  between two dense sub-networks which need to communicate to solve the task leads
  to functional specialization of the sub-networks, but only at extreme levels of
  sparsity. With even a moderate number of interconnections, the sub-networks become
  functionally entangled. Defining functional specialization is in itself a challenging
  problem without a universally agreed solution. To address this, we designed three
  different measures of specialization (based on weight masks, retraining and correlation)
  and found them to qualitatively agree. Our results have implications in both neuroscience
  and machine learning. For neuroscience, it shows that we cannot conclude that there
  is functional modularity simply by observing moderate levels of structural modularity:
  knowing the brainâ€™s connectome is not sufficient for understanding how it breaks
  down into functional modules. For machine learning, using structure to promote functional
  modularity â€“ which may be important for robustness and generalization â€“ may require
  extremely narrow bottlenecks between modules.'
last_tweet_in_thread: '1405131238160179201'
twitter_thread: >
  <blockquote class="twitter-tweet" data-width="400" data-dnt="true"><p lang="en" dir="ltr">New preprint/tweeprint! ðŸ§µðŸ‘‡<br><br>Modularity can be structural (what connects to what) or functional (specialised groups of neurons). Are these related? Yes, but more weakly than you might guess.<br><br>Work by PhD student Gabriel BÃ©na - feedback appreciated!<a href="https://t.co/h70TXa7jFT">https://t.co/h70TXa7jFT</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131198834282499?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">TLDR: enforcing structural modularity in the architecture of a NN trained on a task naturally composed of subtasks leads to module specialisation on subtasks, but only at extreme levels. Even quite high levels of structural modularity lead to no functional specialisation.</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131202038804483?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">We looked at the simplest possible case of two modules, each densely connected with sparse connections between them. This lets us precisely control structural modularity from maximum (single interconnect) to no modularity (fully interconnected).</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131206094725125?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">Each module gets a separate input MNIST digit. The whole network has to return one or the other of the digits depending on whether two digit parities same or different. This forces modules to share some info, but allows for specialisation on subtask of classifying each digit.</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131209311756290?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">Now the tricky part: how do we measure whether or not the modules have specialised? This is a surprisingly complicated question - see the replies in this thread: <a href="https://t.co/vS89qj08xy">https://t.co/vS89qj08xy</a><br><br>Thanks to everyone who replied in that thread - your ideas really helped!</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131212386152449?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">We couldn&#39;t find a single satisfactory answer, so we tried three separate measures of functional specialisation (based on informational bottleneck, weight masks, correlation). Fortunately, they all qualitatively agreed in this case, so it seems they are measuring something real.</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131216572006404?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">Results: functional specialisation increases monotonically with structural modularity Q, but only becomes substantial when Q is close to max (Q=0.5). If there are more than a handful of connections between modules, they become functionally entangled. <a href="https://t.co/MFAKV3WAoN">pic.twitter.com/MFAKV3WAoN</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131225854005256?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">So what&#39;s the take home message here? Firstly, it&#39;s great that these three very different measures broadly agree. Secondly, if you&#39;re looking to use structural modularity as a proxy for functional modularity, beware! You only get that at extreme levels.</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131229834383364?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">For reference, a network with a structural modularity Q=0.35 is usually described as &quot;highly modular&quot;, but in all of our measures doesn&#39;t give rise to much functional specialisation at all. Does this pose problems for the connectomics project?</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131232812408832?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">One thing we&#39;d like to look into in future is how this changes when the number of bits of information that the modules need to share changes. In our task, they only need to share one bit.<br><br>Any other ideas or comments/questions?<br><br>Thanks for reading!</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131235496767496?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">Also, I&#39;m using this as an opportunity to share the &quot;notpaper&quot; version of this preprint. It&#39;s an experimental new way of reading papers I&#39;m working on as a side project. Would be interested in feedback on this too - do you find it helpful?<a href="https://t.co/zx5ifDT3do">https://t.co/zx5ifDT3do</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1405131238160179201?ref_src=twsrc%5Etfw">June 16, 2021</a></blockquote>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>