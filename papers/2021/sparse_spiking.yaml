year: 2021
last_updated: 12-1-2022
authors: Perez-Nieves N, Goodman DFM
title: Sparse spiking gradient descent
journal: Advances in Neural Information Processing Systems
additional: '34'
categories:
- Machine learning
- Spiking
- Neural simulation
- Learning
urls:
- - Journal
  - https://proceedings.neurips.cc/paper/2021/hash/61f2585b0ebcf1f532c4d1ec9a7d51aa-Abstract.html
- - PDF
  - https://proceedings.neurips.cc/paper/2021/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf
abstract: |-
  There is an increasing interest in emulating Spiking Neural Networks (SNNs) on neuromorphic
  computing devices due to their low energy consumption. Recent advances have allowed training
  SNNs to a point where they start to compete with traditional Artificial Neural Networks (ANNs)
  in terms of accuracy, while at the same time being energy efficient when run on neuromorphic
  hardware. However, the process of training SNNs is still based on dense tensor operations
  originally developed for ANNs which do not leverage the spatiotemporally sparse nature of SNNs.
  We present here the first sparse SNN backpropagation algorithm which achieves the same or better
  accuracy as current state of the art methods while being significantly faster and more memory
  efficient. We show the effectiveness of our method on real datasets of varying complexity
  (Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a speedup in the
  backward pass of up to 150x, and 85% more memory efficient, without losing accuracy.
