selected: true
year: 2021
last_updated: 4-10-2021
authors: Perez-Nieves N, Leung VCH, Dragotti PL, Goodman DFM
title: Neural heterogeneity promotes robust learning
journal: Nature Communications
additional: 12, 5791
doi: 10.1038/s41467-021-26022-3
categories:
- Neuroscience
- Learning
- Visual
- Auditory
- Spiking
- Machine learning
- Modelling
urls:
- - Journal (HTML)
  - https://www.nature.com/articles/s41467-021-26022-3
- - Journal (PDF)
  - https://www.nature.com/articles/s41467-021-26022-3.pdf
- - Preprint
  - https://www.biorxiv.org/content/10.1101/2020.12.18.423468v3
- - Code (GitHub)
  - https://github.com/npvoid/neural_heterogeneity
- - Code (Zenodo)
  - https://zenodo.org/record/5413181
- - Neurotheory talk by Dan Goodman (video)
  - https://www.youtube.com/watch?v=V2HFqVfeTPg&feature=youtu.be
- - SNUFA 2021 talk by Nicolas Perez (video)
  - https://youtu.be/aLNGcs_zYsY
- - Podcast with WaterCooler Neuroscience
  - https://open.spotify.com/episode/6PYxjRFg2bI8pSDN82Dume?si=S7Z4BxXNS2aRCnbBMskTHg
- - Twitter
  - https://twitter.com/neuralreckoning/status/1341011316975218695
abstract: The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity
  at the neural level plays a functional role remains unclear, and has been relatively
  little explored in models which are often highly homogeneous. We compared the performance
  of spiking neural networks trained to carry out tasks of real-world difficulty,
  with varying degrees of heterogeneity, and found that heterogeneity substantially
  improved task performance. Learning with heterogeneity was more stable and robust,
  particularly for tasks with a rich temporal structure. In addition, the distribution
  of neuronal parameters in the trained networks is similar to those observed experimentally.
  We suggest that the heterogeneity observed in the brain may be more than just the
  byproduct of noisy processes, but rather may serve an active and important role
  in allowing animals to learn in changing environments.
last_tweet_in_thread: '1341011376299511809'
video_embed: <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/aLNGcs_zYsY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
twitter_thread: >
 <blockquote class="twitter-tweet" data-width="400" data-dnt="true"><p lang="en" dir="ltr">Have you ever wondered why neurons are like snowflakes? No two alike, even if they&#39;re the same type. In our latest preprint, we think we have (at least part of) the answer: it promotes robust learning.<br><br>Tweeprint follows (1/16)<a href="https://t.co/GbN9dXxt7Q">https://t.co/GbN9dXxt7Q</a> <a href="https://t.co/1IQ1dvUYoZ">pic.twitter.com/1IQ1dvUYoZ</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011316975218695?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">One of the striking things about the brain is how much diversity there is at so many levels, e.g. the distribution of membrane time constants. Check these out from <a href="https://twitter.com/AllenInstitute?ref_src=twsrc%5Etfw">@AllenInstitute</a> and <a href="https://twitter.com/AuditoryNeuro?ref_src=twsrc%5Etfw">@AuditoryNeuro</a> - wide range of values for single cell types, bit like a log normal dist. (2/16) <a href="https://t.co/O5gT0GSwPa">pic.twitter.com/O5gT0GSwPa</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011323434504193?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">But, most models of networks of neurons that can carry out complex information processing tasks tend to use the same parameters for all neurons of the same type, with typically only connectivity being different for each neuron. (3/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011326945136642?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">We guessed that networks of neurons with a wide range of time constants would be better at tasks where information is present at multiple time scales, such as auditory tasks like recognising speech. (4/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011329587527682?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">Since we were interested in temporal dynamics and comparing to biology, we used spiking neural networks trained using <a href="https://twitter.com/hisspikeness?ref_src=twsrc%5Etfw">@hisspikeness</a> <a href="https://twitter.com/virtualmind?ref_src=twsrc%5Etfw">@virtualmind</a> surrogate gradient descent, adapted to learn time constants as well as weights. <a href="https://t.co/vq9BTqJ05F">https://t.co/vq9BTqJ05F</a> (5/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011332175454212?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">We found no improvement for N-MNIST which has little to no useful temporal information in the stimuli, some improvement for DVS gestures which does have temporal info but can be solved well without using it, and a huge improvement for recognising spoken digits. (6/16) <a href="https://t.co/tdwmP86uNd">pic.twitter.com/tdwmP86uNd</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011337527386113?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">The distributions of time constants learned are consistent for a given task, for each run, and regardless of whether you initialise time constants randomly or at a fixed value. And, they look like the distributions you find in the brain. (7/16) <a href="https://t.co/z32g3RtNuC">pic.twitter.com/z32g3RtNuC</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011342921228289?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">And it&#39;s more robust. If you tune hyperparameters for sounds at a single speed, and then change the playback speed of stimuli, the heterogeneous networks can still learn the task but homogeneous ones start to fall down. (8/16) <a href="https://t.co/Otwkdon6Pb">pic.twitter.com/Otwkdon6Pb</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011348130590720?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">We also tested another training method, spiking FORCE of Nicola and <a href="https://twitter.com/ClopathLab?ref_src=twsrc%5Etfw">@ClopathLab</a> and found the same robustness to hyperparameter mistuning. This fig shows the region where learning converges to a good solution in blue, axes are hyperparameters. (9/16) <a href="https://t.co/WfK1e66hD8">pic.twitter.com/WfK1e66hD8</a></p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011354275209216?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">So this looks to be pretty general: heterogeneity improves learning of temporal tasks and gives robustness against hyperparameter mistuning. And it does so at no metabolic cost. Same performance with homog networks requires 10x more neurons! So, surely the brain uses this (10/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011357282541576?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">This is also good for neuromorphic computing and ML: adding neuron level heterogeneity costs only O(n) time and memory, whereas adding more neurons and synapses costs O(nÂ²). (11/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011359723630593?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">That&#39;s it for the results, we hope that this spurs people to look further into the importance of heterogeneity, e.g. spatial or cell type heterogeneity, and whether it can be useful in ML too. (12/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011362777083904?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">This work was done by two super talented PhD students Nicolas Perez and Vincent Cheung, neither of whom are on twitter unfortunately. (13/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011365729845259?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">And if you&#39;re interested in this area, there&#39;s a fascinating literature on heterogeneity, briefly reviewed in this paper, including similar work with a more neuromorphic angle from <a href="https://twitter.com/SanderBohte?ref_src=twsrc%5Etfw">@SanderBohte</a>, Tim Masquelier. Nice review in <a href="https://twitter.com/GjorJulijana?ref_src=twsrc%5Etfw">@GjorJulijana</a> <a href="https://t.co/VAsL9rDWmR">https://t.co/VAsL9rDWmR</a> (14/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011369265672192?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">This work was only possible thanks to two incredible scientific developments. The first is new methods of training spiking neural networks from <a href="https://twitter.com/hisspikeness?ref_src=twsrc%5Etfw">@hisspikeness</a> and others. We had a workshop on this over the summer, videos available at <a href="https://t.co/sNMvHiZsI3">https://t.co/sNMvHiZsI3</a> (15/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011373036367872?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <blockquote class="twitter-tweet" data-conversation="none" data-width="400" data-dnt="true"><p lang="en" dir="ltr">The second is the availability of incredible experimental datasets thanks to orgs like <a href="https://twitter.com/AllenInstitute?ref_src=twsrc%5Etfw">@AllenInstitute</a>, <a href="https://t.co/jjSbYPf3T6">https://t.co/jjSbYPf3T6</a> (<a href="https://twitter.com/neuronJoy?ref_src=twsrc%5Etfw">@neuronJoy</a> <a href="https://twitter.com/rgerkin?ref_src=twsrc%5Etfw">@rgerkin</a>) and individual labs like <a href="https://twitter.com/AuditoryNeuro?ref_src=twsrc%5Etfw">@AuditoryNeuro</a>. Thank you so much for your generosity! (16/16)</p>&mdash; Dan Goodman (@neuralreckoning) <a href="https://twitter.com/neuralreckoning/status/1341011376299511809?ref_src=twsrc%5Etfw">December 21, 2020</a></blockquote>
 <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>