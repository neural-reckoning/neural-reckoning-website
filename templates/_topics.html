<h2>Themes and suggested topics</h2>

<p>
    Currently, the main focus of our research is on understanding how features of biological
    brains contribute to "intelligent" behaviour. In particular, we are interested in the role
    of resource constraints (energy, space, time). We want to solve these questions not only
    to understand brains, but also because answers to questions about how the brain has
    solved these problems may throw light on how we can design synthetic "intelligent" systems
    that are able to operate with limited resources. This naturally ties in with an interest
    in neuromorphic computing, the application of brain-like ideas to the design of new
    energy-efficient computing devices. A thread that runs through all this work is
    the use of spiking neural networks, both because this is the mechanism used by the brain,
    but also because it has shown considerable potential in a neuromorphic computing setup.
</p>

<p>
    Here are some examples of these types of questions that we have either worked on
    previously or are planning on working on. But, don't let this list or the paragraph
    above stop you from getting in contact with other related ideas!
</p>

<ul class="list-group">
    <li class="list-group-item">
        <b>Modularity and specialisation.</b>
        Different parts of the brain have relatively sparse
        interconnections, and yet they are able to work together as a whole. How is this
        possible? Is it related to the way that some areas of the brain seem to have
        specialised functions? Are there advantages to this from a learning or
        resource efficiency point of view?
        <br/><a href="pub_sparsity_specialization.html">[Sample publication]</a>
    </li>
    <li class="list-group-item">
        <b>Relating low-level mechanisms to high-level function.</b>
        The brain uses a very rich array of different low level mechanisms, from
        the structure of neurons, down to different types of ion channels. Which
        of these mechanisms are important for simulating and understanding the
        brain as a whole?
        <br/><a href="pub_heterogeneity.html">[Sample publication on heterogeneity]</a>
        <a href="pub_neuromodulation_enhances_sensory.html">[Sample publication on neuromodulation]</a>
        <a href="pub_heterogeneous_delays_low_bit.html">[Sample publication on delay learning]</a>
        <a href="pub_multimodal.html">[Sample publication on the role of nonlinearity in multimodal integration]</a>
    </li>
    <li class="list-group-item">
        <b>Spiking neurons.</b> The brain uses discrete but precisely timed "spikes"
        instead of the continuous activations used in neural networks. Does this have
        computational or resource efficiency advantages? How can we train networks with
        these discontinuities?
        <br/><a href="pub_beyond_rate_coding.html">[Sample publication on rate versus time in SNNs]</a>
        <a href="pub_sparse_spiking_gradient_descent.html">[Sample publication on sparsity in training SNNs]</a>
    </li>
    <li class="list-group-item">
        <b>Neuromorphic computing.</b> How do we use all these to design better
        hardware that can carry out "intelligent" workloads with reduced energy
        requirements?
        <br/><a href="pub_dual_memory_pathways.html">[Sample publication on algorithm-hardware co-design]</a>
    </li>

</ul>